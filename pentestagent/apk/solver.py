"""APK CTF Solver - Deterministic pattern-based solver for APK reverse engineering CTFs."""

import base64
import binascii
import json
import os
import re
import subprocess
import sys
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Optional, Callable


# Default flag patterns
DEFAULT_FLAG_PATTERNS = [
    r"picoCTF\{[^}]+\}",
    r"flag\{[^}]+\}",
    r"CTF\{[^}]+\}",
    r"FLAG\{[^}]+\}",
]

# Keywords that hint at encoded/hidden data
HINT_KEYWORDS = [
    "flag", "secret", "key", "password", "token", "hidden",
    "encode", "decode", "base32", "base64", "rot", "xor", "hex",
    "banana", "cipher", "encrypted", "obfuscated",
]

# Regex for potential encoded tokens
# Base64: A-Za-z0-9+/= with length >= 16
BASE64_PATTERN = r"[A-Za-z0-9+/]{16,}={0,2}"
# Base32: A-Z2-7= with length >= 16
BASE32_PATTERN = r"[A-Z2-7]{16,}={0,8}"
# Hex: even number of hex chars >= 16
HEX_PATTERN = r"(?:[0-9A-Fa-f]{2}){8,}"


@dataclass
class FlagEvidence:
    """Evidence for a found flag."""
    flag: str
    file: str
    line: int
    extract: str
    method: str
    input_token: str


@dataclass
class PatternHit:
    """A pattern match found during scanning."""
    file: str
    line: int
    pattern_type: str  # "keyword", "base64_token", "base32_token", "hex_token", "flag_direct"
    match: str
    context: str


@dataclass
class DecodeAttempt:
    """Record of a decode attempt."""
    token: str
    method: str
    success: bool
    result: Optional[str] = None
    error: Optional[str] = None


@dataclass
class SolverResult:
    """Result from the solver."""
    success: bool
    flags: list = field(default_factory=list)  # List of FlagEvidence
    hits: list = field(default_factory=list)  # List of PatternHit
    decode_attempts: list = field(default_factory=list)  # List of DecodeAttempt
    stop_reason: Optional[str] = None
    elapsed_seconds: float = 0.0


class TranscriptLogger:
    """Logger that writes to transcript.txt with timestamps."""

    def __init__(self, transcript_path: Path):
        self.path = transcript_path
        self.start_time = datetime.now()
        # Clear/create file
        self.path.write_text(f"# APK CTF Solver Transcript\n# Started: {self.start_time.isoformat()}\n\n")

    def _timestamp(self) -> str:
        elapsed = (datetime.now() - self.start_time).total_seconds()
        return f"[{elapsed:07.3f}s]"

    def _write(self, tag: str, message: str):
        line = f"{self._timestamp()} [{tag}] {message}\n"
        with open(self.path, "a") as f:
            f.write(line)
        # Also write to stderr for visibility (won't break protocols on stdout)
        print(line.rstrip(), file=sys.stderr)

    def event(self, message: str):
        self._write("EVENT", message)

    def action(self, message: str):
        self._write("ACTION", message)

    def hit(self, message: str):
        self._write("HIT", message)

    def decode(self, message: str):
        self._write("DECODE", message)

    def result(self, message: str):
        self._write("RESULT", message)

    def error(self, message: str):
        self._write("ERROR", message)


def is_printable_text(data: bytes) -> bool:
    """Check if decoded bytes look like printable text."""
    try:
        text = data.decode("utf-8")
        # Allow printable ASCII + common whitespace
        return all(c.isprintable() or c in "\n\r\t" for c in text)
    except UnicodeDecodeError:
        return False


def decode_base64(token: str) -> Optional[str]:
    """Try to decode as base64."""
    try:
        # Normalize padding
        padded = token + "=" * (4 - len(token) % 4) if len(token) % 4 else token
        decoded = base64.b64decode(padded, validate=True)
        if is_printable_text(decoded):
            return decoded.decode("utf-8")
    except Exception:
        pass
    return None


def decode_base32(token: str) -> Optional[str]:
    """Try to decode as base32."""
    try:
        # Normalize: uppercase, fix padding
        upper = token.upper()
        padded = upper + "=" * (8 - len(upper) % 8) if len(upper) % 8 else upper
        decoded = base64.b32decode(padded)
        if is_printable_text(decoded):
            return decoded.decode("utf-8")
    except Exception:
        pass
    return None


def decode_hex(token: str) -> Optional[str]:
    """Try to decode as hex."""
    try:
        decoded = binascii.unhexlify(token)
        if is_printable_text(decoded):
            return decoded.decode("utf-8")
    except Exception:
        pass
    return None


def decode_rot13(token: str) -> str:
    """Decode ROT13."""
    result = []
    for c in token:
        if "a" <= c <= "z":
            result.append(chr((ord(c) - ord("a") + 13) % 26 + ord("a")))
        elif "A" <= c <= "Z":
            result.append(chr((ord(c) - ord("A") + 13) % 26 + ord("A")))
        else:
            result.append(c)
    return "".join(result)


def decode_xor_single_byte(data: bytes, key: int) -> bytes:
    """XOR with single byte key."""
    return bytes(b ^ key for b in data)


def try_xor_bruteforce(token: str, flag_patterns: list, max_key: int = 256) -> Optional[tuple]:
    """Try XOR brute force with single-byte keys."""
    # Try as raw bytes first
    try:
        data = token.encode("utf-8")
    except Exception:
        return None

    for key in range(1, max_key):
        decoded = decode_xor_single_byte(data, key)
        if is_printable_text(decoded):
            text = decoded.decode("utf-8")
            for pattern in flag_patterns:
                if re.search(pattern, text):
                    return (text, f"xor_key_{key}")

    # Also try if token looks like hex
    try:
        data = binascii.unhexlify(token)
        for key in range(1, max_key):
            decoded = decode_xor_single_byte(data, key)
            if is_printable_text(decoded):
                text = decoded.decode("utf-8")
                for pattern in flag_patterns:
                    if re.search(pattern, text):
                        return (text, f"hex_xor_key_{key}")
    except Exception:
        pass

    return None


# Decoder registry: name -> (function, applicable_pattern)
DECODERS = [
    ("base64", decode_base64, BASE64_PATTERN),
    ("base32", decode_base32, BASE32_PATTERN),
    ("hex", decode_hex, HEX_PATTERN),
    ("rot13", decode_rot13, None),  # None = try on any token
]


def scan_file_for_patterns(
    file_path: Path,
    base_dir: Path,
    flag_patterns: list,
    logger: Optional[TranscriptLogger] = None,
) -> tuple:
    """
    Scan a file for patterns and potential encoded tokens.

    Returns:
        (direct_flags, hits): direct flag matches and pattern hits
    """
    direct_flags = []
    hits = []

    try:
        content = file_path.read_text(errors="ignore")
    except Exception as e:
        if logger:
            logger.error(f"Cannot read {file_path}: {e}")
        return direct_flags, hits

    rel_path = str(file_path.relative_to(base_dir))
    lines = content.split("\n")

    for line_num, line in enumerate(lines, 1):
        # Check for direct flag matches
        for pattern in flag_patterns:
            for match in re.finditer(pattern, line):
                flag = match.group(0)
                direct_flags.append(FlagEvidence(
                    flag=flag,
                    file=rel_path,
                    line=line_num,
                    extract=line.strip()[:200],
                    method="direct_match",
                    input_token=flag,
                ))
                if logger:
                    logger.hit(f"Direct flag match in {rel_path}:{line_num}: {flag}")

        # Check for hint keywords
        line_lower = line.lower()
        for keyword in HINT_KEYWORDS:
            if keyword in line_lower:
                hits.append(PatternHit(
                    file=rel_path,
                    line=line_num,
                    pattern_type="keyword",
                    match=keyword,
                    context=line.strip()[:200],
                ))

        # Check for Base64-like tokens
        for match in re.finditer(BASE64_PATTERN, line):
            token = match.group(0)
            # Skip if it's just alphanumeric noise (all same case, no mixed)
            if len(set(token)) > 4:  # Has some variety
                hits.append(PatternHit(
                    file=rel_path,
                    line=line_num,
                    pattern_type="base64_token",
                    match=token[:50] + ("..." if len(token) > 50 else ""),
                    context=line.strip()[:200],
                ))

        # Check for Base32-like tokens
        for match in re.finditer(BASE32_PATTERN, line):
            token = match.group(0)
            if len(set(token)) > 4:
                hits.append(PatternHit(
                    file=rel_path,
                    line=line_num,
                    pattern_type="base32_token",
                    match=token[:50] + ("..." if len(token) > 50 else ""),
                    context=line.strip()[:200],
                ))

        # Check for hex-like tokens
        for match in re.finditer(HEX_PATTERN, line):
            token = match.group(0)
            hits.append(PatternHit(
                file=rel_path,
                line=line_num,
                pattern_type="hex_token",
                match=token[:50] + ("..." if len(token) > 50 else ""),
                context=line.strip()[:200],
            ))

    return direct_flags, hits


def extract_tokens_from_hits(hits: list) -> list:
    """Extract unique tokens from pattern hits for decoding attempts."""
    tokens = set()
    for hit in hits:
        if hit.pattern_type in ("base64_token", "base32_token", "hex_token"):
            # Get full token from context if truncated
            token = hit.match.rstrip("...")
            if len(token) >= 16:
                tokens.add((token, hit.pattern_type, hit.file, hit.line, hit.context))
    return list(tokens)


def try_decode_token(
    token: str,
    token_type: str,
    flag_patterns: list,
    logger: Optional[TranscriptLogger] = None,
) -> tuple:
    """
    Try various decoders on a token.

    Returns:
        (flag_or_none, method_or_none, attempts_list)
    """
    attempts = []

    for decoder_name, decoder_func, applicable_pattern in DECODERS:
        # Skip if decoder doesn't apply to this token type
        if applicable_pattern:
            if token_type == "base64_token" and decoder_name != "base64":
                if decoder_name not in ("rot13",):
                    continue
            if token_type == "base32_token" and decoder_name != "base32":
                if decoder_name not in ("rot13",):
                    continue
            if token_type == "hex_token" and decoder_name != "hex":
                if decoder_name not in ("rot13",):
                    continue

        if logger:
            logger.decode(f"Trying {decoder_name} on token: {token[:30]}...")

        try:
            result = decoder_func(token)
            if result:
                attempts.append(DecodeAttempt(
                    token=token[:50],
                    method=decoder_name,
                    success=True,
                    result=result[:100] if result else None,
                ))

                # Check if result contains a flag
                for pattern in flag_patterns:
                    match = re.search(pattern, result)
                    if match:
                        if logger:
                            logger.result(f"FLAG FOUND via {decoder_name}: {match.group(0)}")
                        return (match.group(0), decoder_name, attempts)
            else:
                attempts.append(DecodeAttempt(
                    token=token[:50],
                    method=decoder_name,
                    success=False,
                    error="No valid output",
                ))
        except Exception as e:
            attempts.append(DecodeAttempt(
                token=token[:50],
                method=decoder_name,
                success=False,
                error=str(e)[:100],
            ))

    # Try XOR brute force as last resort (bounded)
    if logger:
        logger.decode(f"Trying XOR brute force on token: {token[:30]}...")

    xor_result = try_xor_bruteforce(token, flag_patterns, max_key=256)
    if xor_result:
        text, method = xor_result
        attempts.append(DecodeAttempt(
            token=token[:50],
            method=method,
            success=True,
            result=text[:100],
        ))
        match = re.search("|".join(flag_patterns), text)
        if match:
            if logger:
                logger.result(f"FLAG FOUND via {method}: {match.group(0)}")
            return (match.group(0), method, attempts)
    else:
        attempts.append(DecodeAttempt(
            token=token[:50],
            method="xor_bruteforce",
            success=False,
            error="No flag found in XOR results",
        ))

    return (None, None, attempts)


def scan_directory_for_tokens(
    directory: Path,
    base_dir: Path,
    extensions: list,
    flag_patterns: list,
    logger: Optional[TranscriptLogger] = None,
    max_files: int = 1000,
) -> tuple:
    """
    Scan a directory for patterns.

    Returns:
        (direct_flags, all_hits)
    """
    all_flags = []
    all_hits = []
    files_scanned = 0

    for ext in extensions:
        for file_path in directory.rglob(f"*{ext}"):
            if files_scanned >= max_files:
                if logger:
                    logger.event(f"Reached max files limit ({max_files})")
                break

            if logger:
                logger.action(f"Scanning {file_path.relative_to(base_dir)}")

            flags, hits = scan_file_for_patterns(file_path, base_dir, flag_patterns, logger)
            all_flags.extend(flags)
            all_hits.extend(hits)
            files_scanned += 1

        if files_scanned >= max_files:
            break

    return all_flags, all_hits


class APKSolver:
    """Deterministic APK CTF solver."""

    def __init__(
        self,
        out_dir: Path,
        flag_regex: Optional[str] = None,
        max_decode_attempts: int = 100,
        max_files: int = 2000,
    ):
        self.out_dir = out_dir
        self.apktool_dir = out_dir / "apktool_out"
        self.jadx_dir = out_dir / "jadx_out"
        self.transcript_path = out_dir / "transcript.txt"

        # Flag patterns
        if flag_regex:
            self.flag_patterns = [flag_regex]
        else:
            self.flag_patterns = DEFAULT_FLAG_PATTERNS

        self.max_decode_attempts = max_decode_attempts
        self.max_files = max_files
        self.logger: Optional[TranscriptLogger] = None

    def solve(self) -> SolverResult:
        """
        Run the solver on extracted APK artifacts.

        Expects apktool_out/ and jadx_out/ to already exist.
        """
        import time
        start_time = time.time()

        # Initialize logger
        self.logger = TranscriptLogger(self.transcript_path)
        self.logger.event("Solver started")

        result = SolverResult(success=False)
        all_flags = []
        all_hits = []
        all_attempts = []

        try:
            # Phase 1: Scan resources (highest priority)
            self.logger.event("Phase 1: Scanning resources")

            # Check strings.xml and other value XMLs first
            values_dir = self.apktool_dir / "res" / "values"
            if values_dir.exists():
                self.logger.action("Scanning res/values/ XMLs")
                flags, hits = scan_directory_for_tokens(
                    values_dir, self.out_dir, [".xml"],
                    self.flag_patterns, self.logger, self.max_files
                )
                all_flags.extend(flags)
                all_hits.extend(hits)

            # Early exit if direct flag found
            if all_flags:
                self.logger.result(f"Direct flag(s) found in resources: {len(all_flags)}")
                result.success = True
                result.flags = all_flags
                result.hits = all_hits
                result.elapsed_seconds = time.time() - start_time
                return result

            # Phase 2: Scan all resources
            self.logger.event("Phase 2: Scanning all resource XMLs")
            res_dir = self.apktool_dir / "res"
            if res_dir.exists():
                flags, hits = scan_directory_for_tokens(
                    res_dir, self.out_dir, [".xml"],
                    self.flag_patterns, self.logger, self.max_files
                )
                all_flags.extend(flags)
                all_hits.extend(hits)

            # Phase 3: Scan assets
            self.logger.event("Phase 3: Scanning assets")
            assets_dir = self.apktool_dir / "assets"
            if assets_dir.exists():
                flags, hits = scan_directory_for_tokens(
                    assets_dir, self.out_dir, ["", ".txt", ".json", ".xml", ".html", ".js"],
                    self.flag_patterns, self.logger, self.max_files
                )
                all_flags.extend(flags)
                all_hits.extend(hits)

            # Phase 4: Scan raw resources
            self.logger.event("Phase 4: Scanning raw resources")
            raw_dir = self.apktool_dir / "res" / "raw"
            if raw_dir.exists():
                flags, hits = scan_directory_for_tokens(
                    raw_dir, self.out_dir, [""],
                    self.flag_patterns, self.logger, self.max_files
                )
                all_flags.extend(flags)
                all_hits.extend(hits)

            # Phase 5: Scan decompiled Java sources
            self.logger.event("Phase 5: Scanning decompiled Java sources")
            sources_dir = self.jadx_dir / "sources"
            if sources_dir.exists():
                flags, hits = scan_directory_for_tokens(
                    sources_dir, self.out_dir, [".java"],
                    self.flag_patterns, self.logger, self.max_files
                )
                all_flags.extend(flags)
                all_hits.extend(hits)

            # Check for direct flags
            if all_flags:
                self.logger.result(f"Direct flag(s) found: {len(all_flags)}")
                result.success = True
                result.flags = all_flags
                result.hits = all_hits
                result.elapsed_seconds = time.time() - start_time
                return result

            # Phase 6: Try decoding tokens
            self.logger.event("Phase 6: Attempting to decode tokens")

            # Extract tokens from hits
            tokens = extract_tokens_from_hits(all_hits)
            self.logger.action(f"Found {len(tokens)} candidate tokens to decode")

            # Sort by token type priority (base32 often used in CTFs)
            tokens.sort(key=lambda x: (
                0 if x[1] == "base32_token" else
                1 if x[1] == "base64_token" else
                2
            ))

            decode_count = 0
            for token, token_type, file, line, context in tokens:
                if decode_count >= self.max_decode_attempts:
                    self.logger.event(f"Reached max decode attempts ({self.max_decode_attempts})")
                    result.stop_reason = "max_decode_attempts"
                    break

                # Re-extract full token from context if needed
                full_token = token
                if token_type == "base32_token":
                    match = re.search(BASE32_PATTERN, context)
                    if match:
                        full_token = match.group(0)
                elif token_type == "base64_token":
                    match = re.search(BASE64_PATTERN, context)
                    if match:
                        full_token = match.group(0)
                elif token_type == "hex_token":
                    match = re.search(HEX_PATTERN, context)
                    if match:
                        full_token = match.group(0)

                flag, method, attempts = try_decode_token(
                    full_token, token_type, self.flag_patterns, self.logger
                )
                all_attempts.extend(attempts)
                decode_count += 1

                if flag:
                    evidence = FlagEvidence(
                        flag=flag,
                        file=file,
                        line=line,
                        extract=context[:200],
                        method=method,
                        input_token=full_token[:100] + ("..." if len(full_token) > 100 else ""),
                    )
                    all_flags.append(evidence)
                    self.logger.result(f"FLAG FOUND: {flag}")
                    result.success = True
                    break

            if not all_flags:
                self.logger.result("No flag found after all attempts")
                if not result.stop_reason:
                    result.stop_reason = "exhausted_all_strategies"

            result.flags = all_flags
            result.hits = all_hits[:100]  # Limit stored hits
            result.decode_attempts = all_attempts[:100]  # Limit stored attempts
            result.success = len(all_flags) > 0

        except Exception as e:
            self.logger.error(f"Solver error: {e}")
            result.stop_reason = f"error: {e}"

        result.elapsed_seconds = time.time() - start_time
        self.logger.event(f"Solver finished in {result.elapsed_seconds:.2f}s")

        return result
