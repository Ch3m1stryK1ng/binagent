"""
BinAgent - Unified LLM-driven security agent.

Two modes only:
- analyze: vulnerability detection / reverse engineering analysis
- solve: CTF-style solving (goal is to recover/validate a flag)

Core principle: Plan → Act → Observe → Re-plan loop driven by LLM.
Planning is MANDATORY before any tool calls.
"""

import json
import logging
import re
import string
from difflib import SequenceMatcher
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, List, Optional, AsyncIterator

from .base_agent import BaseAgent, AgentMessage, ToolCall
from .state import AgentState

if TYPE_CHECKING:
    from ..knowledge import RAGEngine
    from ..llm import LLM
    from ..runtime import Runtime
    from ..tools import Tool

logger = logging.getLogger("pentestagent.bin_agent")


class AgentMode(str, Enum):
    """BinAgent operating modes."""
    ANALYZE = "analyze"  # Vulnerability detection / RE analysis
    SOLVE = "solve"      # CTF-style flag finding


@dataclass
class PlanStep:
    """A single step in the agent's plan."""
    number: int
    description: str
    tool_hint: str  # Which tool(s) to use
    evidence_goal: str  # What output confirms/denies hypothesis
    status: str = "pending"  # pending, complete, skipped, failed
    result: Optional[str] = None


@dataclass
class TaskContext:
    """Context for a task to be solved by BinAgent."""

    # Mode (required)
    mode: AgentMode = AgentMode.ANALYZE

    # Task description (required)
    description: str = ""

    # Optional file to analyze
    file_path: Optional[str] = None

    # Optional network target
    host: Optional[str] = None
    port: Optional[int] = None

    # Flag pattern for solve mode
    flag_regex: str = r"picoCTF\{[^}]+\}|flag\{[^}]+\}|CTF\{[^}]+\}"

    # Artifacts
    run_id: str = field(default_factory=lambda: datetime.now().strftime("%Y%m%d_%H%M%S"))
    out_dir: Optional[str] = None

    def to_dict(self) -> dict:
        return {
            "mode": self.mode.value,
            "description": self.description,
            "file_path": self.file_path,
            "host": self.host,
            "port": self.port,
            "flag_regex": self.flag_regex,
            "run_id": self.run_id,
        }


# System prompt for BinAgent - matches the spec
BINAGENT_PROMPT = '''You are BinAgent, a general-purpose LLM-driven security agent.

## Mode: {mode}
{mode_description}

## Current Task
{task_description}

## Task Context
{task_context}

## Available Tools
{tools_list}

## Ghidra MCP Status
{ghidra_status}

## WORKFLOW

### 1. Planning Phase
Start with a numbered plan (typically 3-7 steps) whenever possible.
Each step must be:
- **Actionable**: specify which tool you will use
- **Evidence-oriented**: state what output will confirm/deny your hypothesis

Format your plan as:
```
PLAN:
1. [Tool: tool_name] Description - Evidence: what confirms/denies
2. [Tool: tool_name] Description - Evidence: what confirms/denies
...
```

### 2. Execution Phase
After outlining your plan, execute step-by-step:
- Call the tool for current step
- Extract observations from output
- Mark step complete
- Proceed to next step or re-plan if stuck

### 3. Re-planning (when stuck)
If progress stalls (no new evidence, repeated failures, tool errors):
1. State explicitly WHY you are stuck
2. Propose 2-3 alternative hypotheses
3. Choose one and continue with tools

Do NOT loop blindly or spam random payloads.
If repeated tool calls are not producing new evidence, summarize what is known and move forward.

## Rules
- ALWAYS call tools to gather information. Never guess or fabricate outputs.
- Use Ghidra MCP tools when available for binary analysis (imports, decompilation, xrefs).
- Record evidence: file:line, function:offset, or transcript snippets.
- Keep responses concise. Focus on tool calls and brief analysis.

## Analyze Mode Constraints (CRITICAL)
When mode is ANALYZE, your plan MUST be dangerous-callsite driven.
If preflight evidence exists, keep it SHORT (2–4 steps).
Otherwise use 5–7 steps:
1) Identify binary + mitigations (file/checksec/readelf).
2) Identify risky imports/sinks/sources (readelf/objdump/Ghidra symbols).
3) Find xrefs/callers for risky imports or key strings.
4) Decompile ONLY the caller functions for those risky calls.
5) Map evidence to CWEs with function + address + callsite snippet.
6) If time allows, confirm additional sinks/sources.
7) Summarize findings.
Do NOT dump full disassembly. Only include small evidence windows (20–60 lines).
Keep tool calls ≤ 12 for small binaries.

At the end, output a JSON block named FINDINGS_JSON with:
[
  {{"cwe":"CWE-134","title":"Format string","function":"sub_x","address":"0x...","evidence":"<snippet>","rationale":"...","confidence":"high/medium/low"}}
]

## Success Criteria
{success_criteria}

Begin by outputting your PLAN, then execute it step by step.
'''


class GeneralAgent(BaseAgent):
    """
    BinAgent - Unified LLM-driven security agent.

    Two modes:
    - analyze: vulnerability detection, produce security report
    - solve: CTF solving, find and validate flags

    Workflow: Plan → Act → Observe → Re-plan
    Planning is MANDATORY and code-enforced.
    """

    def __init__(
        self,
        llm: "LLM",
        tools: List["Tool"],
        runtime: "Runtime",
        context: TaskContext,
        rag_engine: Optional["RAGEngine"] = None,
        **kwargs,
    ):
        """Initialize BinAgent."""
        super().__init__(llm, tools, runtime, **kwargs)
        self.context = context
        self.rag_engine = rag_engine

        # Set up output directory
        self.context.out_dir = str(Path("runs") / self.context.run_id)
        Path(self.context.out_dir).mkdir(parents=True, exist_ok=True)
        self.set_llm_trace_path(str(Path(self.context.out_dir) / "llm_trace.jsonl"))

        # Plan tracking
        self.plan: List[PlanStep] = []
        self.plan_generated = False
        self.plan_generation_attempts = 0
        self.used_fallback_plan = False

        # Structured logging
        self.tool_log: List[Dict[str, Any]] = []
        self.evidence: List[Dict[str, Any]] = []
        self.transcript: List[str] = []
        self._tool_args: Dict[str, Any] = {}

        # Results tracking
        self.flags_found: List[str] = []
        self.findings: List[Dict[str, Any]] = []

        # Ghidra tools status
        self._ghidra_available = False
        self._ghidra_tools: List[str] = []
        self._ghidra_binary_name: Optional[str] = None
        self._preflight_done = False

        # Parse network target from description if needed
        if not self.context.host and "nc " in self.context.description.lower():
            self._parse_nc_from_description()

        logger.info(f"[BIN_AGENT] Initialized. Mode: {self.context.mode.value}")
        logger.info(f"[BIN_AGENT] Run ID: {self.context.run_id}")

    def should_auto_plan(self) -> bool:
        """GeneralAgent manages its own planning."""
        return False

    def _parse_nc_from_description(self):
        """Extract host:port from description if present."""
        pattern = r'nc\s+([a-zA-Z0-9.-]+)\s+(\d+)'
        match = re.search(pattern, self.context.description)
        if match:
            self.context.host = match.group(1)
            self.context.port = int(match.group(2))

    def check_ghidra_tools(self) -> bool:
        """Check if Ghidra MCP tools are available."""
        required = ["import_binary", "decompile_function", "search_strings", "list_cross_references"]
        tool_names = [t.name.lower() for t in self.tools]

        self._ghidra_tools = []
        for req in required:
            for name in tool_names:
                if req in name:
                    self._ghidra_tools.append(name)
                    break

        self._ghidra_available = len(self._ghidra_tools) >= 3
        return self._ghidra_available

    def get_system_prompt(self, mode: str = "agent") -> str:
        """Generate the system prompt with task context."""
        self.check_ghidra_tools()

        # Build tools list
        tools_list = []
        for tool in self.tools:
            if tool.enabled:
                tools_list.append(f"- {tool.name}: {tool.description[:100]}")

        # Build context string
        context_parts = []
        if self.context.file_path:
            context_parts.append(f"File: {self.context.file_path}")
        if self.context.host and self.context.port:
            context_parts.append(f"Network Target: {self.context.host}:{self.context.port}")

        # Mode-specific description and success criteria
        if self.context.mode == AgentMode.ANALYZE:
            mode_description = (
                "Goal: Produce a security analysis report.\n"
                "- Identify vulnerability classes with evidence\n"
                "- Describe dataflow and security-relevant behavior\n"
                "- Record evidence (file/function/offset, tool outputs)\n"
                "- Include CWE identifiers when applicable\n"
                "- Prioritize dangerous libc calls: strcpy/strcat/gets/scanf/printf/sprintf/snprintf/"
                "vsnprintf/memcpy/memmove/malloc/free/realloc and __*_chk variants"
            )
            success_criteria = (
                "Deliver: findings list with CWE, function, address, evidence snippet, "
                "and mitigations/security posture (NX/PIE/RELRO/canary) if binary."
            )
        else:  # SOLVE
            mode_description = (
                "Goal: Obtain the correct flag and provide evidence trail.\n"
                "- Treat description as puzzle spec\n"
                "- Reverse engineer files if provided (decompiled pseudocode may be provided above)\n"
                "- Study program logic: input handling, transformations, checks, encoded constants\n"
                "- Decode/transform content when needed (XOR, base64, hex, custom encodings)\n"
                "- Interact with services if needed (netcat)\n"
                "- Run the binary with crafted input if needed\n"
                "- For complex challenges (timing attacks, brute-force, crypto, PRNG prediction),\n"
                "  use solve_script to write and execute a full Python solver program\n"
                "- For large non-binary files, use analyze_file FIRST to get structural overview\n"
                "- PATTERN RECOGNITION: If analyze_file or preflight shows many template/function\n"
                "  definitions with arithmetic (add/sub/mod), memory ops (set/index), conditionals,\n"
                "  and I/O operations, the file likely implements a virtual machine or interpreter\n"
                "  (e.g., Brainfuck encoded in Helm templates, Turing machine in config files, etc.).\n"
                "  Strategy: use solve_script to parse the file, extract the encoded program,\n"
                "  build an interpreter/emulator, and determine the expected input.\n"
                f"- Flag pattern: {self.context.flag_regex}"
            )
            success_criteria = (
                f"Deliver: the flag matching {self.context.flag_regex} "
                "+ 2-5 bullet evidence chain of how it was found."
            )

        # Ghidra status
        if self._ghidra_available:
            ghidra_status = f"AVAILABLE: {', '.join(self._ghidra_tools[:5])}"
        else:
            ghidra_status = (
                "NOT AVAILABLE (reduced fidelity mode)\n"
                "Use alternative tools: file, strings, readelf, objdump, etc."
            )

        return BINAGENT_PROMPT.format(
            mode=self.context.mode.value.upper(),
            mode_description=mode_description,
            task_description=self.context.description or "(No description provided)",
            task_context="\n".join(context_parts) if context_parts else "(No additional context)",
            tools_list="\n".join(tools_list[:25]),
            ghidra_status=ghidra_status,
            success_criteria=success_criteria,
        )

    def _log(self, tag: str, message: str):
        """Log to transcript."""
        ts = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        line = f"[{ts}] [{tag}] {message}"
        self.transcript.append(line)
        logger.info(f"[BIN_AGENT] {line}")

    def _log_tool_call(self, tool_name: str, args: Dict[str, Any], result: str, error: Optional[str] = None):
        """Log a tool call to structured tool_log."""
        # Redact sensitive args
        safe_args = {k: ("***" if "pass" in k.lower() or "secret" in k.lower() else v)
                     for k, v in args.items()}

        entry = {
            "timestamp": datetime.now().isoformat(),
            "tool": tool_name,
            "args": safe_args,
            "result_preview": result[:500] if result else None,
            "error": error,
        }
        self.tool_log.append(entry)

    def _add_evidence(self, evidence_type: str, location: str, content: str, metadata: Optional[Dict] = None):
        """Add an evidence entry."""
        entry = {
            "type": evidence_type,  # file, function, network, decoded
            "location": location,   # file:line, func:offset, host:port
            "content": content[:1000],
            "timestamp": datetime.now().isoformat(),
        }
        if metadata:
            entry["metadata"] = metadata
        self.evidence.append(entry)

    def _validate_plan(self, plan_text: str) -> tuple[bool, List[PlanStep], str]:
        """
        Validate a plan from LLM output.

        Returns:
            (is_valid, parsed_steps, error_message)
        """
        # Look for PLAN: section
        plan_match = re.search(r'PLAN:\s*\n((?:\d+\..*\n?)+)', plan_text, re.IGNORECASE)
        if not plan_match:
            # Try alternative format without PLAN: header
            plan_match = re.search(r'(?:^|\n)(\d+\.\s+\[Tool:.*(?:\n\d+\.\s+\[Tool:.*)*)', plan_text, re.MULTILINE)

        if not plan_match:
            return False, [], "No valid plan found. Expected 'PLAN:' followed by numbered steps."

        plan_section = plan_match.group(1) if plan_match else plan_text

        # Parse steps
        step_pattern = r'(\d+)\.\s*(?:\[Tool:\s*([^\]]+)\])?\s*([^-\n]+)(?:\s*-\s*Evidence:\s*(.+))?'
        steps = []

        for match in re.finditer(step_pattern, plan_section):
            num = int(match.group(1))
            tool_hint = match.group(2) or "unspecified"
            description = match.group(3).strip()
            evidence_goal = match.group(4).strip() if match.group(4) else "verify output"

            steps.append(PlanStep(
                number=num,
                description=description,
                tool_hint=tool_hint,
                evidence_goal=evidence_goal,
            ))

        # Validate count
        min_steps = 5 if self.context.mode == AgentMode.ANALYZE else 3
        if len(steps) < min_steps:
            return False, steps, f"Plan has only {len(steps)} steps. Need at least {min_steps}."
        max_steps = 9 if self.context.mode == AgentMode.SOLVE else 7
        if len(steps) > max_steps:
            return False, steps[:max_steps], f"Plan has {len(steps)} steps. Truncating to {max_steps}."

        # Validate each step has tool hint
        for step in steps:
            if step.tool_hint == "unspecified":
                return False, steps, f"Step {step.number} missing tool specification. Use [Tool: tool_name]."

        # Analyze mode must include xrefs + disassembly steps
        if self.context.mode == AgentMode.ANALYZE:
            tool_hints = " ".join(s.tool_hint.lower() for s in steps)
            if "xrefs" not in tool_hints:
                return False, steps, "Analyze plan missing xrefs step."
            if "disassemble" not in tool_hints:
                return False, steps, "Analyze plan missing disassembly step."

        return True, steps, ""

    async def _generate_plan(self) -> bool:
        """
        Generate and validate the mandatory plan.

        Returns True if valid plan was generated, False if fallback was used.
        """
        max_retries = 2
        self.plan_generation_attempts = 0

        # If we already ran preflight for analyze, keep the plan short.
        # Preflight has already collected decompiled pseudocode in conversation.
        # The LLM just needs to analyze it and produce structured findings.
        if self.context.mode == AgentMode.ANALYZE and self._preflight_done:
            self.plan = [
                PlanStep(
                    number=1,
                    description="Analyze the decompiled pseudocode above for ALL dangerous patterns: "
                                "buffer overflows (strcpy/strcat/memcpy into fixed buffers), "
                                "format string bugs (printf with user-controlled arg), "
                                "use-after-free, double-free, command injection (system() with user input), "
                                "integer overflows (arithmetic that wraps before malloc), "
                                "heap overflows (memcpy with attacker-controlled size into malloc'd buffer), "
                                "out-of-bounds read/write (array index beyond bounds), "
                                "off-by-one errors (loop bounds using <= instead of <), "
                                "null pointer dereference (pointer used without null check), "
                                "and path traversal (user input concatenated into file paths). "
                                "IMPORTANT: A single function may contain MULTIPLE independent vulnerabilities - "
                                "report each one separately with its own CWE. For example, a function that calls "
                                "strcat then system() has both a buffer overflow AND command injection.",
                    tool_hint="notes",
                    evidence_goal="List of vulnerable functions with addresses and dangerous calls",
                ),
                PlanStep(
                    number=2,
                    description="For each vulnerability found, produce a FINDINGS_JSON array. "
                                "Each entry must have: cwe, title, function, address, evidence (code snippet), "
                                "rationale, and confidence (high/medium/low). Output the JSON block. "
                                "Use precise CWEs: CWE-121 (stack overflow), CWE-122 (heap overflow), "
                                "CWE-78 (command injection), CWE-134 (format string), CWE-190 (integer overflow), "
                                "CWE-416 (use-after-free), CWE-415 (double-free), CWE-125 (OOB read), "
                                "CWE-787 (OOB write), CWE-193 (off-by-one), CWE-476 (null deref), "
                                "CWE-22 (path traversal).",
                    tool_hint="notes",
                    evidence_goal="FINDINGS_JSON with CWE, function, address, evidence, rationale",
                ),
                PlanStep(
                    number=3,
                    description="Summarize binary mitigations (NX/PIE/RELRO/canary from readelf output) "
                                "and list all findings with their CWE identifiers.",
                    tool_hint="finish",
                    evidence_goal="Concise security report with mitigations + findings list",
                ),
            ]
            self.plan_generated = True
            self.used_fallback_plan = False
            self._sync_plan_to_runtime()
            return True

        if self.context.mode == AgentMode.SOLVE and self._preflight_done:
            self.plan = [
                PlanStep(number=1,
                    description="Study the preflight analysis above. Identify the challenge type: "
                        "Is this a binary RE challenge, a crypto/encoding puzzle, a network service, "
                        "or a non-binary format (template VM, custom interpreter, encoded program)? "
                        "If preflight shows many template/function definitions with arithmetic and "
                        "memory operations, this is likely a VM — note that and plan accordingly.",
                    tool_hint="analyze_file, read_file, list_dir",
                    evidence_goal="Challenge type identified + key structural observations"),
                PlanStep(number=2,
                    description="Based on challenge type, gather targeted evidence. "
                        "For binaries: study decompiled pseudocode, input handling, checks, constants. "
                        "For VMs/interpreters: read key sections to understand the instruction set "
                        "(what operations map to what behavior). "
                        "For crypto: identify algorithm, keys, ciphertext.",
                    tool_hint="read_file, search_pattern, run_command",
                    evidence_goal="Program logic or VM instruction mapping understood"),
                PlanStep(number=3,
                    description="Determine solution strategy and implement it. "
                        "For simple transforms: use run_command or python_eval. "
                        "For complex challenges (VMs, timing, brute-force, crypto, PRNG): "
                        "use solve_script to write a comprehensive Python solver. "
                        "For VMs: the solver should parse the file, extract the encoded program, "
                        "build an interpreter, and determine expected input.",
                    tool_hint="run_command, python_eval, decode, solve_script",
                    evidence_goal="Solver output with candidate flag"),
                PlanStep(number=4,
                    description="If the solver didn't find the flag, iterate: debug the script, "
                        "adjust the approach, or try alternative methods. "
                        "Use solve_script again with fixes if needed.",
                    tool_hint="solve_script, run_command",
                    evidence_goal="Refined solver output or alternative approach result"),
                PlanStep(number=5,
                    description=f"Verify the flag matches the expected pattern ({self.context.flag_regex}). "
                        "If not yet found, try running the binary/program with the crafted input. "
                        "Record the flag and evidence chain.",
                    tool_hint="run_command, finish",
                    evidence_goal=f"Flag matching {self.context.flag_regex} + evidence chain"),
            ]
            self.plan_generated = True
            self.used_fallback_plan = False
            self._sync_plan_to_runtime()
            return True

        plan_range = "5-7" if self.context.mode == AgentMode.ANALYZE else "3-9"
        planning_prompt = self._build_initial_message() + f"\n\nProvide an initial plan ({plan_range} numbered steps with [Tool: ...] and Evidence), then execute it."

        for attempt in range(max_retries + 1):
            self.plan_generation_attempts += 1
            self._log("PLAN", f"Generating plan (attempt {attempt + 1}/{max_retries + 1})")

            response = await self.llm.generate(
                system_prompt=self.get_system_prompt(),
                messages=[{"role": "user", "content": planning_prompt}],
                tools=None,  # No tools during planning
            )

            if not response.content:
                self._log("PLAN", "Empty response from LLM")
                continue

            is_valid, steps, error = self._validate_plan(response.content)

            if is_valid:
                self.plan = steps
                self.plan_generated = True
                self._log("PLAN", f"Valid plan with {len(steps)} steps")
                for step in steps:
                    self._log("PLAN", f"  {step.number}. [{step.tool_hint}] {step.description}")
                # Sync plan to runtime for finish tool
                self._sync_plan_to_runtime()
                return True

            # Invalid - prepare reprompt
            self._log("PLAN", f"Invalid plan: {error}")
            planning_prompt = (
                f"Your previous plan was invalid: {error}\n\n"
                "Please provide a valid plan with:\n"
                f"- {plan_range} numbered steps\n"
                "- Each step has [Tool: tool_name] specification\n"
                "- Each step has 'Evidence: what confirms/denies'\n\n"
                f"Task: {self.context.description}\n\n"
                "PLAN:"
            )

        # All attempts failed - use fallback
        self._log("PLAN", "All plan attempts failed. Using fallback plan.")
        self.used_fallback_plan = True
        self.plan = self._create_fallback_plan()
        self.plan_generated = True
        # Sync plan to runtime for finish tool
        self._sync_plan_to_runtime()
        return False

    def _create_fallback_plan(self) -> List[PlanStep]:
        """Create a minimal fallback plan based on available inputs."""
        steps = []

        if self.context.mode == AgentMode.ANALYZE:
            return self._create_analyze_plan()

        # Step 1: Identify inputs
        if self.context.file_path:
            steps.append(PlanStep(
                number=1,
                description="Identify file type and basic properties",
                tool_hint="file, strings",
                evidence_goal="Determine if binary/APK/archive and architecture",
            ))
        elif self.context.host and self.context.port:
            steps.append(PlanStep(
                number=1,
                description="Connect to service and gather banner/protocol info",
                tool_hint="netcat",
                evidence_goal="Understand service behavior and expected input format",
            ))
        else:
            steps.append(PlanStep(
                number=1,
                description="Analyze task description for clues",
                tool_hint="terminal",
                evidence_goal="Extract actionable information from description",
            ))

        # Step 2: Gather more info
        if self._ghidra_available and self.context.file_path:
            steps.append(PlanStep(
                number=2,
                description="Import binary and search for symbols",
                tool_hint="mcp_ghidra-local_import_binary, mcp_ghidra-local_search_symbols_by_name",
                evidence_goal="Identify interesting functions to analyze",
            ))
        else:
            steps.append(PlanStep(
                number=2,
                description="Extract strings and search for patterns",
                tool_hint="strings, grep",
                evidence_goal="Find flags, URLs, interesting constants",
            ))

        # Step 3: Mode-specific action
        if self.context.mode == AgentMode.SOLVE:
            steps.append(PlanStep(
                number=3,
                description="Search for flag pattern or decode candidates",
                tool_hint="decode, regex search",
                evidence_goal=f"Find flag matching {self.context.flag_regex}",
            ))
        else:
            steps.append(PlanStep(
                number=3,
                description="Identify vulnerability indicators",
                tool_hint="Ghidra decompile or strings analysis",
                evidence_goal="Find dangerous functions, unsafe patterns",
            ))

        return steps

    def _create_analyze_plan(self) -> List[PlanStep]:
        """Create a dangerous-callsite-driven analyze plan."""
        return [
            PlanStep(
                number=1,
                description="Identify binary type and mitigations (file/checksec/readelf)",
                tool_hint="run_command",
                evidence_goal="ELF class/arch + RELRO/NX/PIE/Canary",
            ),
            PlanStep(
                number=2,
                description="Identify risky imports/sinks (strcpy/strcat/printf/scanf/memcpy/malloc/free and __*_chk)",
                tool_hint="run_command",
                evidence_goal="Imported symbol list includes risky libc calls",
            ),
            PlanStep(
                number=3,
                description="Import binary in Ghidra and search strings for clues",
                tool_hint="mcp_ghidra-local_import_binary, mcp_ghidra-local_search_strings",
                evidence_goal="Ghidra loaded + strings show prompts/mode hints/format strings",
            ),
            PlanStep(
                number=4,
                description="Find xrefs/callers for risky imports and suspicious strings",
                tool_hint="mcp_ghidra-local_list_cross_references",
                evidence_goal="Caller functions identified for risky imports",
            ),
            PlanStep(
                number=5,
                description="Decompile only the caller functions for risky calls (small evidence windows)",
                tool_hint="mcp_ghidra-local_decompile_function",
                evidence_goal="Callsite snippets show unsafe usage patterns",
            ),
            PlanStep(
                number=6,
                description="Map evidence to CWEs and produce findings JSON",
                tool_hint="analysis",
                evidence_goal="FINDINGS_JSON with CWE, function, address, evidence, rationale",
            ),
        ]

    def _sync_plan_to_runtime(self):
        """Sync our plan to runtime.plan for the finish tool."""
        from ..tools.finish import TaskPlan, PlanStep as FinishPlanStep, StepStatus

        # Convert our plan steps to finish tool format
        finish_steps = []
        for step in self.plan:
            finish_step = FinishPlanStep(
                id=step.number,
                description=f"[{step.tool_hint}] {step.description} - Evidence: {step.evidence_goal}",
                status=StepStatus.PENDING,
                result=None,
            )
            finish_steps.append(finish_step)

        # Create TaskPlan and attach to runtime
        task_plan = TaskPlan(
            steps=finish_steps,
            original_request=self.context.description,
        )
        self.runtime.plan = task_plan
        # Keep self._task_plan in sync so base_agent._run_loop() checks
        # the correct plan (not the old empty one where is_complete() == True)
        self._task_plan = task_plan
        self._log("PLAN", f"Synced {len(finish_steps)} steps to runtime.plan")

    def _build_initial_message(self) -> str:
        """Build the initial message for the agent."""
        parts = [f"Task: {self.context.description}"]

        if self.context.file_path:
            parts.append(f"\nFile to analyze: {self.context.file_path}")

        if self.context.host and self.context.port:
            parts.append(f"\nNetwork target: {self.context.host}:{self.context.port}")

        if self.context.mode == AgentMode.SOLVE:
            parts.append(f"\nGoal: Find the flag (pattern: {self.context.flag_regex})")
        else:
            parts.append("\nGoal: Produce vulnerability analysis report with CWE labels and evidence.")

        return "\n".join(parts)

    def _check_for_flags(self, text: str):
        """Check text for flag patterns."""
        for match in re.finditer(self.context.flag_regex, text):
            flag = match.group(0)
            if flag not in self.flags_found:
                self.flags_found.append(flag)
                self._log("FLAG", f"Found: {flag}")
                self._add_evidence("flag", "extracted", flag)

    async def solve(self, on_message=None) -> Dict[str, Any]:
        """
        Main entry point: run the agent.

        Workflow:
        1. Generate and validate plan (MANDATORY)
        2. Execute plan step-by-step
        3. Re-plan if stuck
        4. Save artifacts

        Args:
            on_message: Optional callback ``(phase: str, data: dict) -> None``
                invoked for live progress updates.  *phase* is one of
                ``"preflight"``, ``"plan"``, ``"thinking"``, ``"tool_call"``,
                ``"tool_result"``, ``"complete"``.

        Returns:
            Dict with success, flags/findings, artifacts
        """
        from ..observability import create_trace, flush as lf_flush, span_context

        _emit = on_message or (lambda phase, data: None)
        self._on_message = _emit  # Expose to preflight()

        self._log("EVENT", f"Starting BinAgent in {self.context.mode.value} mode")
        self._log("EVENT", f"Description: {self.context.description[:100]}...")

        start_time = datetime.now()

        # Create Langfuse root trace for this solve run (overrides base agent_loop trace)
        self._lf_trace = create_trace(
            name=f"solve_{self.context.mode.value}",
            session_id=f"run_{self.context.run_id}",
            metadata={
                "mode": self.context.mode.value,
                "run_id": self.context.run_id,
                "file_path": self.context.file_path,
            },
            tags=["binagent", self.context.mode.value],
        )

        # Phase 0: Reset and preflight (gathers evidence before planning)
        # We intentionally avoid agent_loop reset later so preflight evidence
        # remains in conversation history for the LLM.
        self.reset()
        self.state_manager.transition_to(AgentState.THINKING)
        _emit("preflight", {"status": "starting"})
        try:
            with span_context(self._lf_trace, "preflight"):
                await self.preflight()
            _emit("preflight", {"status": "done", "tool_calls": len(self.tool_log)})
        except Exception:
            _emit("preflight", {"status": "error"})

        # Phase 1: Planning
        self._log("PHASE", "=== PLANNING PHASE ===")
        _emit("plan", {"status": "generating"})
        plan_valid = await self._generate_plan()
        if not plan_valid:
            self._log("WARN", "Using fallback plan - analysis may be limited")

        # Save plan artifact immediately
        self._save_plan_artifact()
        _emit("plan", {
            "status": "ready",
            "steps": [{"number": s.number, "description": s.description} for s in self.plan],
            "fallback": self.used_fallback_plan,
        })

        # Phase 2: Execution
        self._log("PHASE", "=== EXECUTION PHASE ===")
        execution_message = self._build_execution_message()
        # Add execution message after preflight evidence
        self.conversation_history.append(AgentMessage(role="user", content=execution_message))
        stagnation_ticks = 0
        last_findings_count = len(self.findings)
        # If we already have findings but keep making no progress, stop early.
        stagnation_limit = 12

        try:
            async for msg in self._run_loop():
                progress_made = False
                # Log messages
                if msg.content:
                    self._log("AGENT", msg.content[:300])
                    _emit("thinking", {"content": msg.content})

                # Log tool calls
                if msg.tool_calls:
                    for tc in msg.tool_calls:
                        self._log("TOOL_CALL", f"{tc.name}({json.dumps(tc.arguments)[:200]})")
                        if tc.id:
                            self._tool_args[tc.id] = tc.arguments
                        _emit("tool_call", {
                            "name": tc.name,
                            "arguments": tc.arguments,
                        })

                # Process tool results
                if msg.tool_results:
                    for tr in msg.tool_results:
                        result_preview = (tr.result or tr.error or "")[:300]
                        self._log("TOOL_RESULT", f"{tr.tool_name}: {result_preview}")

                        # Log to structured tool_log
                        self._log_tool_call(
                            tr.tool_name,
                            self._tool_args.get(tr.tool_call_id, {}),
                            tr.result or "",
                            tr.error
                        )

                        _emit("tool_result", {
                            "name": tr.tool_name,
                            "success": tr.success,
                            "result": (tr.result or "")[:500],
                            "error": tr.error,
                        })

                        # Check for flags
                        if tr.result:
                            self._check_for_flags(tr.result)

                            # Extract evidence from results
                            if "0x" in tr.result or "function" in tr.result.lower():
                                self._add_evidence("function", tr.tool_name, tr.result[:500])

                            if "decompile_function" in tr.tool_name.lower():
                                self._add_evidence("pseudocode", tr.tool_name, tr.result[:1000])

                            # Extract structured findings from tool output (analyze mode)
                            if self.context.mode == AgentMode.ANALYZE:
                                self._extract_findings_from_tool_result(tr.tool_name, tr.result)

                        # Step state transitions indicate progress.
                        if tr.tool_name == "finish" and (tr.result or ""):
                            lower_result = tr.result.lower()
                            if "step " in lower_result and (
                                "complete" in lower_result
                                or "skipped" in lower_result
                                or "failed" in lower_result
                            ):
                                progress_made = True

                # Also extract findings from tool call arguments (e.g., notes tool value)
                if msg.tool_calls and self.context.mode == AgentMode.ANALYZE:
                    for tc in msg.tool_calls:
                        if tc.arguments:
                            # Check 'value' field of notes calls for CWE mentions
                            val = tc.arguments.get("value", "")
                            if val and ("CWE-" in val or "FINDINGS_JSON" in val):
                                self._extract_findings_from_content(val)

                # Check for flags in agent content
                if msg.content:
                    self._check_for_flags(msg.content)

                    # Check for findings (analyze mode)
                    if self.context.mode == AgentMode.ANALYZE:
                        self._extract_findings_from_content(msg.content)

                # New findings count as progress.
                current_findings_count = len(self.findings)
                if current_findings_count > last_findings_count:
                    progress_made = True
                    last_findings_count = current_findings_count

                if progress_made:
                    stagnation_ticks = 0
                else:
                    stagnation_ticks += 1

                if (
                    self.context.mode == AgentMode.ANALYZE
                    and last_findings_count > 0
                    and stagnation_ticks >= stagnation_limit
                ):
                    self._log(
                        "WARN",
                        f"Stopping early after {stagnation_ticks} stagnant turns with {last_findings_count} findings."
                    )
                    break

        except Exception as e:
            self._log("ERROR", f"Agent loop error: {e}")
            import traceback
            traceback.print_exc()

        elapsed = (datetime.now() - start_time).total_seconds()
        self._log("EVENT", f"Task complete. Elapsed: {elapsed:.2f}s")

        # Save all artifacts
        result = self._save_artifacts(elapsed)

        # Flush Langfuse events before returning
        lf_flush()

        return result

    def _build_execution_message(self) -> str:
        """Build the execution message with the plan."""
        parts = [self._build_initial_message()]

        parts.append("\n\n## YOUR PLAN (execute this):")
        for step in self.plan:
            parts.append(f"{step.number}. [{step.tool_hint}] {step.description} - Evidence: {step.evidence_goal}")

        parts.append("\n\nNow execute Step 1. Call the appropriate tool.")

        return "\n".join(parts)

    def _extract_findings_from_content(self, content: str):
        """Extract vulnerability findings from agent content."""
        # Ignore planning text to avoid treating CWE lists in plans/prompts as findings.
        upper = (content or "").upper()
        if (
            upper.strip().startswith("PLAN:")
            or "## YOUR PLAN" in upper
            or "USE PRECISE CWES" in upper
        ):
            return

        # Parse structured findings JSON if present
        if "FINDINGS_JSON" in content:
            try:
                match = re.search(r"FINDINGS_JSON\s*[:=]?\s*```json\s*(.*?)```", content, re.DOTALL)
                if match:
                    data = json.loads(match.group(1))
                    if isinstance(data, list):
                        for f in data:
                            self._add_structured_finding(f)
                    return
            except Exception:
                pass

        content_lower = content.lower()

        # Look for CWE mentions
        cwe_pattern = r'CWE-(\d+)'
        for match in re.finditer(cwe_pattern, content, re.IGNORECASE):
            cwe_id = f"CWE-{match.group(1)}"
            # Extract surrounding context
            start = max(0, match.start() - 100)
            end = min(len(content), match.end() + 200)
            context = content[start:end]

            finding = {
                "cwe": cwe_id,
                "context": context.strip(),
                "timestamp": datetime.now().isoformat(),
            }
            if finding not in self.findings:
                self.findings.append(finding)
                self._log("FINDING", f"Detected {cwe_id}")

        # Also look for vulnerability keywords without CWE
        vuln_patterns = [
            (r'buffer\s+overflow', 'CWE-120'),
            (r'stack\s*[-\s]?based\s+buffer\s+overflow', 'CWE-121'),
            (r'heap\s*[-\s]?based\s+buffer\s+overflow', 'CWE-122'),
            (r'format\s+string', 'CWE-134'),
            (r'use[\s-]?after[\s-]?free|uaf', 'CWE-416'),
            (r'double[\s-]?free', 'CWE-415'),
            (r'integer\s+overflow', 'CWE-190'),
            (r'command\s+injection', 'CWE-78'),
            (r'sql\s+injection', 'CWE-89'),
            (r'path\s+traversal', 'CWE-22'),
            (r'out[\s-]?of[\s-]?bounds', 'CWE-125'),
            (r'null\s+pointer\s+dereference', 'CWE-476'),
            (r'race\s+condition', 'CWE-362'),
            (r'uninitialized\s+(memory|variable|pointer)', 'CWE-457'),
        ]

        for pattern, default_cwe in vuln_patterns:
            for match in re.finditer(pattern, content_lower):
                # Skip negated statements like "no confirmed buffer overflow".
                prefix = content_lower[max(0, match.start() - 60):match.start()]
                if re.search(
                    r"\b(no|not|none|without|never|did not|didn't|cannot|can't|unable)\b",
                    prefix,
                ):
                    continue
                # Check if we already have this finding with a CWE
                start = max(0, match.start() - 100)
                end = min(len(content), match.end() + 200)
                context = content[start:end]

                # Skip if we already logged a CWE for similar context
                already_found = any(
                    default_cwe in f.get("cwe", "") or
                    match.group(0) in f.get("context", "").lower()
                    for f in self.findings
                )
                if not already_found:
                    finding = {
                        "cwe": default_cwe,
                        "vuln_type": match.group(0),
                        "context": context.strip(),
                        "timestamp": datetime.now().isoformat(),
                    }
                    self.findings.append(finding)
                    self._log("FINDING", f"Detected {match.group(0)} ({default_cwe})")

    def _extract_findings_from_tool_result(self, tool_name: str, result: str) -> None:
        """Extract findings from tool outputs (Ghidra decompilation/xrefs)."""
        if not result or not tool_name:
            return
        if "disassemble_function" not in tool_name and "decompile_function" not in tool_name:
            return

        try:
            data = json.loads(result)
            if isinstance(data, dict) and "data" in data:
                data = data.get("data", data)
        except Exception:
            return

        func_name = data.get("function_name") or data.get("function") or data.get("name") or ""
        dangerous_calls = data.get("dangerous_calls", []) or []
        disasm = data.get("disassembly", []) or []
        pseudocode = (
            data.get("pseudocode")
            or data.get("decompilation")
            or data.get("code")
            or ""
        )

        cwe_map = {
            "printf": ("CWE-134", "Format string", "User-controlled format string passed to printf"),
            "sprintf": ("CWE-134", "Format string", "User-controlled format string passed to sprintf"),
            "scanf": ("CWE-134", "Format string", "Unbounded/format-sensitive scanf usage"),
            "sscanf": ("CWE-120", "Stack buffer overflow", "Unbounded sscanf into fixed-size buffer"),
            "gets": ("CWE-120", "Stack buffer overflow", "Unbounded input via gets"),
            "strcpy": ("CWE-120", "Stack buffer overflow", "Unbounded copy into fixed buffer"),
            "strcat": ("CWE-120", "Stack buffer overflow", "Unbounded concatenation into fixed buffer"),
            "strncpy": ("CWE-120", "Stack buffer overflow", "Potential missing bounds/termination in strncpy"),
            "memcpy": ("CWE-122", "Heap/stack overflow", "Memcpy with attacker-controlled size"),
            "memmove": ("CWE-122", "Heap/stack overflow", "Memmove with attacker-controlled size"),
            "malloc": ("CWE-190", "Integer overflow", "Allocation size derived from attacker-controlled value"),
            "realloc": ("CWE-190", "Integer overflow", "Allocation size derived from attacker-controlled value"),
            "free": ("CWE-416", "Use-after-free", "Potential dereference after free within same function"),
            "system": ("CWE-78", "Command injection", "User-controlled input passed to system"),
            "fopen": ("CWE-22", "Path traversal", "User-controlled path passed to file open"),
        }

        for call in dangerous_calls:
            func = (call.get("function") or "").lower()
            if not func:
                continue
            mapped = None
            for k, v in cwe_map.items():
                if k in func:
                    mapped = v
                    break
            if not mapped:
                continue

            cwe, title, rationale = mapped
            evidence = call.get("context", "")
            finding = {
                "cwe": cwe,
                "title": title,
                "function": func_name,
                "address": call.get("address", ""),
                "evidence": evidence,
                "rationale": rationale,
                "confidence": "medium",
                "timestamp": datetime.now().isoformat(),
            }
            if finding not in self.findings:
                self.findings.append(finding)
                self._log("FINDING", f"Detected {cwe} from {func_name} ({func})")

        # Simple decompile heuristics for non-sink issues
        try:
            if pseudocode:
                lower = pseudocode.lower()
                # `scanf("%s", buf)` into a fixed stack buffer is a high-signal stack overflow pattern.
                if re.search(r'scanf\s*\(\s*"\s*%s', lower):
                    snippet = ""
                    for line in pseudocode.splitlines():
                        if "scanf" in line and "%s" in line:
                            snippet = line.strip()
                            break
                    finding = {
                        "cwe": "CWE-120",
                        "title": "Unbounded scanf into buffer",
                        "function": func_name,
                        "address": data.get("address", ""),
                        "evidence": snippet or 'scanf("%s", ...)',
                        "rationale": 'Unbounded "%s" format in scanf can overflow destination buffer.',
                        "confidence": "high",
                        "timestamp": datetime.now().isoformat(),
                    }
                    if finding not in self.findings:
                        self.findings.append(finding)
                # printf(variable) without a string literal format is a strong format-string indicator.
                if re.search(r'printf\s*\(\s*[a-z_][a-z0-9_]*\s*\)', lower) and not re.search(r'printf\s*\(\s*"', lower):
                    snippet = ""
                    for line in pseudocode.splitlines():
                        if "printf(" in line and '"' not in line:
                            snippet = line.strip()
                            break
                    finding = {
                        "cwe": "CWE-134",
                        "title": "Potential format string",
                        "function": func_name,
                        "address": data.get("address", ""),
                        "evidence": snippet or "printf(user_controlled_arg)",
                        "rationale": "Non-literal format argument can enable format-string exploitation.",
                        "confidence": "medium",
                        "timestamp": datetime.now().isoformat(),
                    }
                    if finding not in self.findings:
                        self.findings.append(finding)
                if "system(" in lower:
                    self.findings.append({
                        "cwe": "CWE-78",
                        "title": "Command injection",
                        "function": func_name,
                        "address": data.get("address", ""),
                        "evidence": "system(...)",
                        "rationale": "User-controlled input may reach system call.",
                        "confidence": "medium",
                        "timestamp": datetime.now().isoformat(),
                    })
                if "fopen(" in lower and ("\"/tmp\"" in lower or "path" in lower):
                    self.findings.append({
                        "cwe": "CWE-22",
                        "title": "Path traversal",
                        "function": func_name,
                        "address": data.get("address", ""),
                        "evidence": "fopen(path, ...)",
                        "rationale": "User-controlled path passed to file open.",
                        "confidence": "medium",
                        "timestamp": datetime.now().isoformat(),
                    })
                if "free(" in lower and "free(" in lower:
                    # crude double-free heuristic (two frees in same function)
                    if lower.count("free(") >= 2:
                        self.findings.append({
                            "cwe": "CWE-415",
                            "title": "Double free",
                            "function": func_name,
                            "address": data.get("address", ""),
                            "evidence": "free(...) called multiple times",
                            "rationale": "Potential double free in same function.",
                            "confidence": "medium",
                            "timestamp": datetime.now().isoformat(),
                        })
                if "p[" in lower and "== 0" in lower and "p[" in lower:
                    self.findings.append({
                        "cwe": "CWE-476",
                        "title": "Null pointer dereference",
                        "function": func_name,
                        "address": data.get("address", ""),
                        "evidence": "p[0] dereference without null guard",
                        "rationale": "Possible null dereference in pseudocode.",
                        "confidence": "medium",
                        "timestamp": datetime.now().isoformat(),
                    })
                if "s[n+" in lower or "[n+" in lower:
                    self.findings.append({
                        "cwe": "CWE-125",
                        "title": "Out-of-bounds read",
                        "function": func_name,
                        "address": data.get("address", ""),
                        "evidence": "index uses n+const",
                        "rationale": "Potential out-of-bounds read via unchecked index.",
                        "confidence": "medium",
                        "timestamp": datetime.now().isoformat(),
                    })
                if "i <= n" in lower and "[" in lower:
                    self.findings.append({
                        "cwe": "CWE-193",
                        "title": "Off-by-one",
                        "function": func_name,
                        "address": data.get("address", ""),
                        "evidence": "loop uses <= with buffer copy",
                        "rationale": "Potential off-by-one write.",
                        "confidence": "medium",
                        "timestamp": datetime.now().isoformat(),
                    })
        except Exception:
            pass

        # Heuristic UAF: free + later memory access using same register
        try:
            for i, line in enumerate(disasm):
                if line.get("mnemonic") == "call" and "free" in (line.get("operands", "").lower()):
                    # Look ahead for memory access using rdi (free arg)
                    for j in range(i + 1, min(i + 15, len(disasm))):
                        ops = disasm[j].get("operands", "").lower()
                        if "[rdi]" in ops or "rdi+" in ops:
                            finding = {
                                "cwe": "CWE-416",
                                "title": "Use-after-free",
                                "function": func_name,
                                "address": disasm[j].get("address", ""),
                                "evidence": f"{line.get('disasm','')} ; {disasm[j].get('disasm','')}",
                                "rationale": "Memory is accessed after free in the same function (heuristic).",
                                "confidence": "medium",
                                "timestamp": datetime.now().isoformat(),
                            }
                            if finding not in self.findings:
                                self.findings.append(finding)
                                self._log("FINDING", f"Detected CWE-416 heuristic in {func_name}")
                            break
        except Exception:
            pass

        # Heuristic integer overflow / heap overflow: malloc + later size-based writes
        try:
            has_malloc = any(
                l.get("mnemonic") == "call" and "malloc" in (l.get("operands", "").lower())
                for l in disasm
            )
            has_sscanf = any(
                l.get("mnemonic") == "call" and "sscanf" in (l.get("operands", "").lower())
                for l in disasm
            )
            if has_malloc and has_sscanf:
                finding = {
                    "cwe": "CWE-190",
                    "title": "Integer overflow in allocation size",
                    "function": func_name,
                    "address": data.get("start_address", ""),
                    "evidence": "malloc call follows parsed integer input (sscanf) in same function",
                    "rationale": "Allocation size appears derived from attacker-controlled parsed integer; may overflow or be undersized.",
                    "confidence": "medium",
                    "timestamp": datetime.now().isoformat(),
                }
                if finding not in self.findings:
                    self.findings.append(finding)
                    self._log("FINDING", f"Detected CWE-190 heuristic in {func_name}")

                finding2 = {
                    "cwe": "CWE-122",
                    "title": "Heap overflow / out-of-bounds write",
                    "function": func_name,
                    "address": data.get("start_address", ""),
                    "evidence": "malloc call with size derived from input; subsequent loop writes bytes",
                    "rationale": "Potential copy/write beyond allocated buffer when size is attacker-controlled.",
                    "confidence": "medium",
                    "timestamp": datetime.now().isoformat(),
                }
                if finding2 not in self.findings:
                    self.findings.append(finding2)
                    self._log("FINDING", f"Detected CWE-122 heuristic in {func_name}")
        except Exception:
            pass

    def _add_structured_finding(self, f: Dict[str, Any]):
        """Add structured finding from LLM output."""
        if not isinstance(f, dict):
            return
        finding = {
            "cwe": f.get("cwe", "Unknown"),
            "title": f.get("title", ""),
            "function": f.get("function", ""),
            "address": f.get("address", ""),
            "evidence": (f.get("evidence", "") or "")[:1000],
            "rationale": f.get("rationale", ""),
            "confidence": f.get("confidence", "medium"),
            "timestamp": datetime.now().isoformat(),
        }
        if not self._ghidra_available:
            finding["confidence"] = "low"
        if finding not in self.findings:
            self.findings.append(finding)

    async def preflight(self) -> None:
        """Preflight analysis to gather key evidence with minimal tool calls."""
        if self.context.mode not in (AgentMode.ANALYZE, AgentMode.SOLVE):
            return
        if self._preflight_done:
            return

        # Only preflight if we have a file
        if not self.context.file_path:
            return

        # Ensure Ghidra availability state is up to date
        self.check_ghidra_tools()

        # Helper to execute a single tool call and record outputs
        async def _run_tool(name: str, arguments: Dict[str, Any]):
            call_id = f"pre_{len(self.tool_log)+1}"
            tool_calls = [{"id": call_id, "name": name, "arguments": arguments}]
            results = await self._execute_tools(tool_calls)
            if not results:
                return None
            result = results[0]

            # Log tool call + args + result
            self._log("TOOL_CALL", f"{name}({json.dumps(arguments)[:200]})")
            self._log_tool_call(name, arguments, result.result or "", result.error)

            # Add to conversation history for LLM context
            self.conversation_history.append(
                AgentMessage(role="assistant", content="", tool_calls=[ToolCall(id=call_id, name=name, arguments=arguments)])
            )
            self.conversation_history.append(
                AgentMessage(role="tool_result", content="", tool_results=[result])
            )

            # Extract evidence/findings
            if result.result:
                self._add_evidence("function", name, result.result[:500])
                self._extract_findings_from_tool_result(name, result.result)

            return result

        # Tool call budget — Ghidra preflight needs more calls than IDA did:
        # ~2 run_command + 1 import + 1 list_binaries + 1 search_strings +
        # ~9 search_symbols + ~7 xrefs + ~10 decompile callers + 2 list funcs + N decompiles
        budget = 35
        used = 0

        # Step 1: Identify file type
        is_binary = True  # assume binary unless proven otherwise
        if used < budget:
            file_result = await _run_tool("run_command", {"command": f"file {self.context.file_path}", "timeout": 10})
            used += 1
            if file_result and file_result.result:
                file_type_lower = file_result.result.lower()
                is_binary = not any(kw in file_type_lower for kw in [
                    "text", "ascii", "utf-8", "unicode", "script", "source",
                    "xml", "json", "yaml", "html", "python",
                ])

        if is_binary:
            # Binary file path: checksec + readelf + risky symbols
            if used < budget:
                cmd = (
                    f"checksec --file={self.context.file_path} 2>/dev/null || echo 'checksec not found'; "
                    f"echo '---'; "
                    f"readelf -hW {self.context.file_path}; "
                    f"echo '---'; "
                    f"readelf -lW {self.context.file_path};"
                )
                await _run_tool("run_command", {"command": cmd, "timeout": 60})
                used += 1

            if used < budget:
                risky_regex = r"(strcpy|strcat|gets|scanf|printf|sprintf|snprintf|vsnprintf|memcpy|memmove|malloc|realloc|free|__.*_chk)"
                cmd = (
                    f"readelf -sW {self.context.file_path} | egrep -n \"{risky_regex}\" || true"
                )
                await _run_tool("run_command", {"command": cmd, "timeout": 60})
                used += 1
        else:
            # Non-binary file path: structural analysis via analyze_file
            if used < budget:
                analyze_result = await _run_tool("analyze_file", {"path": self.context.file_path})
                used += 1
                # If the file is in a directory with other files, also list the directory
                parent_dir = Path(self.context.file_path).parent
                if used < budget:
                    await _run_tool("list_dir", {"path": str(parent_dir)})
                    used += 1
            # Non-binary files don't need Ghidra — mark preflight done and return
            self._preflight_done = True
            return

        if not self._ghidra_available:
            # Ghidra not available; stop here (LLM may still infer low-confidence findings)
            return

        # Step 3: Import binary into Ghidra + wait for analysis + search strings
        if used < budget:
            import_result = await _run_tool("mcp_ghidra-local_import_binary", {"binary_path": self.context.file_path})
            used += 1
            # Discover actual binary name and wait for Ghidra analysis to finish
            if import_result and import_result.result:
                import asyncio as _aio

                basename = Path(self.context.file_path).stem
                # Poll list_project_binaries until the binary's analysis is
                # complete (or timeout after ~120 seconds).
                max_polls = 24   # 24 × 5s = 120s max wait
                poll_interval = 5
                analysis_ready = False

                for _poll in range(max_polls):
                    await _aio.sleep(poll_interval)
                    list_result = await _run_tool("mcp_ghidra-local_list_project_binaries", {})
                    used += 1
                    if not (list_result and list_result.result):
                        continue
                    try:
                        text = list_result.result
                        data = json.loads(text)
                        programs = data.get("programs", []) if isinstance(data, dict) else data
                        for prog in programs:
                            pname = prog.get("name", "") if isinstance(prog, dict) else str(prog)
                            if basename in pname:
                                self._ghidra_binary_name = pname.lstrip("/")
                                # Check analysis_complete flag
                                if isinstance(prog, dict) and prog.get("analysis_complete"):
                                    analysis_ready = True
                                break
                    except (json.JSONDecodeError, TypeError):
                        # Fallback: line-based parsing
                        for line in text.split("\n"):
                            if basename in line and '"name"' in line:
                                import re as _re
                                m = _re.search(r'"name"\s*:\s*"([^"]+)"', line)
                                if m and basename in m.group(1):
                                    self._ghidra_binary_name = m.group(1).lstrip("/")
                                    break
                    except Exception:
                        pass

                    if analysis_ready:
                        self._log("PREFLIGHT", f"Ghidra analysis complete for {self._ghidra_binary_name} (waited {(_poll + 1) * poll_interval}s)")
                        break
                    else:
                        elapsed_wait = (_poll + 1) * poll_interval
                        self._log("PREFLIGHT", f"Waiting for Ghidra analysis... ({elapsed_wait}s)")
                        _emit_fn = getattr(self, "_on_message", None)
                        if _emit_fn:
                            _emit_fn("preflight", {"status": "ghidra_waiting", "elapsed": elapsed_wait})

                if not analysis_ready:
                    self._log("PREFLIGHT", f"Ghidra analysis not complete after {max_polls * poll_interval}s — proceeding with partial results")

                if not self._ghidra_binary_name:
                    self._ghidra_binary_name = Path(self.context.file_path).name

        strings_result = None
        if used < budget and self._ghidra_binary_name:
            strings_result = await _run_tool("mcp_ghidra-local_search_strings", {"binary_name": self._ghidra_binary_name, "query": ".*", "limit": 200})
            used += 1

        # Step 4: Find risky imports via search_symbols_by_name (gets addresses unlike list_imports)
        risky_keywords = ["strcpy", "strcat", "gets", "scanf", "printf", "sprintf", "memcpy", "system", "free"]
        risky_addrs: List[tuple] = []  # (name, address)
        if used < budget and self._ghidra_binary_name:
            for keyword in risky_keywords:
                if used >= budget:
                    break
                sym_result = await _run_tool("mcp_ghidra-local_search_symbols_by_name", {"binary_name": self._ghidra_binary_name, "query": keyword, "limit": 10})
                used += 1
                if sym_result and sym_result.result:
                    try:
                        data = json.loads(sym_result.result)
                        symbols = data if isinstance(data, list) else data.get("symbols", [])
                        # Pick the PLT thunk entry (lowest non-EXTERNAL address).
                        # PLT thunks live in .text (low addr ~0x1011xx) and are called by user code.
                        # GOT entries live higher (~0x1060xx) and xrefs only show the PLT stub.
                        best = None
                        best_addr_int = float('inf')
                        for sym in symbols:
                            sname = sym.get("name", "")
                            saddr = sym.get("address", "")
                            stype = sym.get("type", "")
                            if stype == "Function" and saddr and not saddr.startswith("EXTERNAL"):
                                try:
                                    addr_int = int(saddr, 16)
                                except (ValueError, TypeError):
                                    continue
                                if addr_int < best_addr_int:
                                    best = (sname, saddr)
                                    best_addr_int = addr_int
                        if best:
                            risky_addrs.append(best)
                    except Exception:
                        pass

        risky_addrs = risky_addrs[:9]

        # Step 5: Xrefs for risky imports (limit) - use address to avoid ambiguity
        callers: List[str] = []
        for func_name, func_addr in risky_addrs:
            if used >= budget:
                break
            # Always use address (from search_symbols_by_name) to avoid ambiguous name matches
            xref_result = await _run_tool("mcp_ghidra-local_list_cross_references", {"binary_name": self._ghidra_binary_name, "name_or_address": func_addr})
            used += 1
            if xref_result and xref_result.result:
                try:
                    data = json.loads(xref_result.result)
                    refs = data if isinstance(data, list) else data.get("cross_references", data.get("references", []))
                    for x in refs:
                        fn = x.get("function_name") or x.get("from_function") or x.get("caller")
                        # Skip null, the thunk itself (e.g., strcpy calling strcpy), and DATA-only refs
                        if fn and fn not in callers and fn != func_name:
                            callers.append(fn)
                except Exception:
                    pass

        # Step 6: Decompile caller functions
        # Reserve budget for Step 7 (FUN_ search + exports + remaining decompiles)
        step7_reserve = 4  # FUN_ search + list_exports + at least 2 decompiles
        decomp_blocks: List[str] = []
        decompiled_set: set = set()  # Track decompiled functions to avoid duplicates
        for caller in callers:
            if used >= budget - step7_reserve:
                break
            if caller in decompiled_set:
                continue
            decomp_result = await _run_tool(
                "mcp_ghidra-local_decompile_function",
                {"binary_name": self._ghidra_binary_name, "name_or_address": caller},
            )
            used += 1
            decompiled_set.add(caller)
            try:
                if decomp_result and decomp_result.result:
                    d = json.loads(decomp_result.result)
                    if isinstance(d, dict) and "data" in d:
                        d = d.get("data", d)
                    pseudocode = d.get("pseudocode") or d.get("decompilation") or d.get("code") or ""
                    fname = d.get("function") or d.get("function_name") or caller
                    if pseudocode:
                        decomp_blocks.append(f"### Decompiled: {fname}\n{pseudocode}")
            except Exception:
                pass

        # For small binaries, decompile all user functions and pass to LLM.
        # Use search_symbols_by_name("FUN_") to find auto-named functions in stripped binaries,
        # then also include exports. This gives much better coverage than exports alone.
        if used < budget and self._ghidra_binary_name:
            try:
                all_func_names: List[str] = []
                # Search for auto-named functions (stripped binaries)
                sym_result = await _run_tool("mcp_ghidra-local_search_symbols_by_name", {"binary_name": self._ghidra_binary_name, "query": "FUN_", "limit": 200})
                used += 1
                if sym_result and sym_result.result:
                    data = json.loads(sym_result.result)
                    symbols = data if isinstance(data, list) else data.get("symbols", [])
                    for sym in symbols:
                        sname = sym.get("name", "")
                        stype = sym.get("type", "")
                        if stype == "Function" and sname and sname not in all_func_names:
                            all_func_names.append(sname)
                # Also add any exports not already in the list
                all_exports = await _run_tool("mcp_ghidra-local_list_exports", {"binary_name": self._ghidra_binary_name})
                used += 1
                if all_exports and all_exports.result:
                    data = json.loads(all_exports.result)
                    funcs = data if isinstance(data, list) else data.get("exports", [])
                    for f in funcs:
                        fname = f.get("name", "") if isinstance(f, dict) else str(f)
                        if fname and not fname.startswith(".") and fname not in all_func_names:
                            all_func_names.append(fname)

                # Filter out trivial init/fini stubs
                skip_prefixes = ("_DT_INIT", "_DT_FINI", "__DT_", "_INIT_0", "_FINI_0")
                candidates = [n for n in all_func_names if not n.startswith(skip_prefixes)]

                # If small, attempt full decompile for all candidates
                if len(candidates) <= 80:
                    # Ensure budget covers all remaining un-decompiled candidates
                    remaining = [c for c in candidates if c not in decompiled_set]
                    budget = max(budget, used + len(remaining) + 2)
                    for fname in candidates:
                        if used >= budget:
                            break
                        if fname in decompiled_set:
                            continue
                        decomp_result = await _run_tool(
                            "mcp_ghidra-local_decompile_function",
                            {"binary_name": self._ghidra_binary_name, "name_or_address": fname},
                        )
                        used += 1
                        decompiled_set.add(fname)
                        try:
                            if decomp_result and decomp_result.result:
                                d = json.loads(decomp_result.result)
                                if isinstance(d, dict) and "data" in d:
                                    d = d.get("data", d)
                                pseudocode = d.get("pseudocode") or d.get("decompilation") or d.get("code") or ""
                                fn = d.get("function") or d.get("function_name") or fname
                                if pseudocode:
                                    decomp_blocks.append(f"### Decompiled: {fn}\n{pseudocode}")
                        except Exception:
                            pass
            except Exception:
                pass

        # Inject decompiled pseudocode blocks into conversation for LLM
        if decomp_blocks:
            joined = "\n\n".join(decomp_blocks)
            max_total = 120000
            if len(joined) > max_total:
                joined = joined[:max_total] + "\n...[truncated pseudocode]..."
            chunk_size = 12000
            chunks = [joined[i:i + chunk_size] for i in range(0, len(joined), chunk_size)]
            for idx, chunk in enumerate(chunks, 1):
                self.conversation_history.append(
                    AgentMessage(
                        role="assistant",
                        content=f"PSEUDOCODE_CHUNK {idx}/{len(chunks)}:\n{chunk}",
                        metadata={"preflight_decompile": True},
                    )
                )

        self._preflight_done = True


    def _save_plan_artifact(self):
        """Save plan.json artifact."""
        out_dir = Path(self.context.out_dir)
        plan_data = {
            "generated_at": datetime.now().isoformat(),
            "attempts": self.plan_generation_attempts,
            "used_fallback": self.used_fallback_plan,
            "steps": [
                {
                    "number": s.number,
                    "description": s.description,
                    "tool_hint": s.tool_hint,
                    "evidence_goal": s.evidence_goal,
                    "status": s.status,
                    "result": s.result,
                }
                for s in self.plan
            ],
        }
        (out_dir / "plan.json").write_text(json.dumps(plan_data, indent=2))

    def _save_artifacts(self, elapsed_seconds: float) -> Dict[str, Any]:
        """Save all artifacts to disk."""
        out_dir = Path(self.context.out_dir)

        # 1. plan.json (already saved, update with final status)
        self._save_plan_artifact()

        # 2. tool_log.json
        (out_dir / "tool_log.json").write_text(json.dumps(self.tool_log, indent=2))

        # 3. evidence.json
        (out_dir / "evidence.json").write_text(json.dumps(self.evidence, indent=2))

        # 4. outcome.json
        result_success = False
        result_findings: List[Dict[str, Any]] = []
        if self.context.mode == AgentMode.SOLVE:
            outcome = {
                "mode": "solve",
                "success": len(self.flags_found) > 0,
                "flags": self.flags_found,
                "evidence_chain": self.evidence[-5:] if self.evidence else [],
            }
            result_success = len(self.flags_found) > 0
        else:
            normalized_findings = [self._normalize_finding(f) for f in self.findings]
            deduped_findings = self._deduplicate_findings(normalized_findings)
            outcome = {
                "mode": "analyze",
                "success": len(deduped_findings) > 0,
                "findings": deduped_findings,
                "ghidra_available": self._ghidra_available,
                "ghidra_tools_used": [e for e in self.tool_log if "ghidra" in e.get("tool", "").lower()],
            }
            result_success = len(deduped_findings) > 0
            result_findings = deduped_findings
        (out_dir / "outcome.json").write_text(json.dumps(outcome, indent=2))

        # 5. transcript.txt
        (out_dir / "transcript.txt").write_text('\n'.join(self.transcript))

        # 6. conversation.md (full conversation history)
        (out_dir / "conversation.md").write_text(self._format_conversation_markdown())

        # 6. run.json (metadata)
        run_data = {
            "run_id": self.context.run_id,
            "mode": self.context.mode.value,
            "started_at": datetime.now().isoformat(),
            "elapsed_seconds": elapsed_seconds,
            "context": self.context.to_dict(),
            "ghidra_available": self._ghidra_available,
            "ghidra_tools": self._ghidra_tools,
            "used_fallback_plan": self.used_fallback_plan,
            "plan_attempts": self.plan_generation_attempts,
            "tools_called": list(set(e.get("tool", "") for e in self.tool_log)),
        }
        (out_dir / "run.json").write_text(json.dumps(run_data, indent=2))

        self._log("EVENT", f"Artifacts saved to {out_dir}")

        return {
            "success": result_success,
            "flags": self.flags_found,
            "findings": result_findings if self.context.mode == AgentMode.ANALYZE else self.findings,
            "run_id": self.context.run_id,
            "out_dir": str(out_dir),
            "elapsed_seconds": elapsed_seconds,
            "ghidra_available": self._ghidra_available,
            "used_fallback_plan": self.used_fallback_plan,
        }

    def _normalize_finding(self, f: Dict[str, Any]) -> Dict[str, Any]:
        """Normalize findings to required schema."""
        if not isinstance(f, dict):
            return {"cwe": "Unknown", "title": "", "function": "", "address": "", "evidence": "", "rationale": "", "confidence": "low"}

        title = f.get("title", "")
        evidence = f.get("evidence", "")
        rationale = f.get("rationale", "")

        # Backfill from legacy fields
        if not title and f.get("vuln_type"):
            title = f.get("vuln_type")
        if not evidence and f.get("context"):
            evidence = f.get("context")
        if not rationale and f.get("context"):
            rationale = "Derived from analysis evidence."

        normalized = {
            "cwe": f.get("cwe", "Unknown"),
            "title": title or "",
            "function": f.get("function", ""),
            "address": f.get("address", ""),
            "evidence": (evidence or "")[:1000],
            "rationale": rationale or "",
            "confidence": f.get("confidence", "medium"),
        }
        if not self._ghidra_available:
            normalized["confidence"] = "low"
        return normalized

    def _normalize_text_for_dedup(self, text: str) -> str:
        """Normalize free text for stable evidence-based deduplication."""
        if not text:
            return ""
        t = text.lower().strip()
        t = re.sub(r"\s+", " ", t)
        t = t.translate(str.maketrans("", "", string.punctuation))
        return t

    def _evidence_similarity(self, a: str, b: str) -> float:
        """Compute lightweight token-overlap similarity between two snippets."""
        a_norm = self._normalize_text_for_dedup(a)
        b_norm = self._normalize_text_for_dedup(b)
        if not a_norm or not b_norm:
            return 0.0
        if a_norm == b_norm or a_norm in b_norm or b_norm in a_norm:
            return 1.0
        a_tokens = set(a_norm.split())
        b_tokens = set(b_norm.split())
        if not a_tokens or not b_tokens:
            return 0.0
        inter = len(a_tokens & b_tokens)
        union = len(a_tokens | b_tokens)
        jaccard = inter / union if union else 0.0
        seq_ratio = SequenceMatcher(None, a_norm, b_norm).ratio()
        return max(jaccard, seq_ratio)

    def _confidence_rank(self, confidence: str) -> int:
        ranks = {"low": 1, "medium": 2, "high": 3}
        return ranks.get((confidence or "").lower(), 1)

    def _merge_findings(self, base: Dict[str, Any], incoming: Dict[str, Any]) -> Dict[str, Any]:
        """Merge two equivalent findings, preserving strongest evidence."""
        merged = dict(base)
        for field in ("title", "function", "address"):
            if not merged.get(field) and incoming.get(field):
                merged[field] = incoming[field]

        base_evidence = merged.get("evidence", "") or ""
        inc_evidence = incoming.get("evidence", "") or ""
        if len(inc_evidence) > len(base_evidence):
            merged["evidence"] = inc_evidence

        base_rationale = merged.get("rationale", "") or ""
        inc_rationale = incoming.get("rationale", "") or ""
        if len(inc_rationale) > len(base_rationale):
            merged["rationale"] = inc_rationale

        if self._confidence_rank(incoming.get("confidence", "")) > self._confidence_rank(merged.get("confidence", "")):
            merged["confidence"] = incoming.get("confidence", merged.get("confidence", "low"))

        return merged

    def _deduplicate_findings(self, findings: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Deduplicate findings using evidence-aware matching.

        Two findings are considered duplicates when they share the same CWE and:
        - same function/address and highly similar evidence, or
        - evidence snippets are near-identical even if function/address are missing.
        """
        deduped: List[Dict[str, Any]] = []

        for finding in findings:
            if not isinstance(finding, dict):
                continue

            cwe = (finding.get("cwe", "") or "").strip()
            fn = (finding.get("function", "") or "").strip()
            addr = (finding.get("address", "") or "").strip()
            evidence = finding.get("evidence", "") or ""

            matched_idx = -1
            for i, existing in enumerate(deduped):
                if (existing.get("cwe", "") or "").strip() != cwe:
                    continue

                ex_fn = (existing.get("function", "") or "").strip()
                ex_addr = (existing.get("address", "") or "").strip()
                ex_ev = existing.get("evidence", "") or ""

                same_location = bool(fn and ex_fn and fn == ex_fn) or bool(addr and ex_addr and addr == ex_addr)
                sim = self._evidence_similarity(evidence, ex_ev)

                # Strict when we have location info, looser when location is missing.
                if (same_location and sim >= 0.45) or (sim >= 0.65):
                    matched_idx = i
                    break

            if matched_idx >= 0:
                deduped[matched_idx] = self._merge_findings(deduped[matched_idx], finding)
            else:
                deduped.append(dict(finding))

        return deduped

    def _format_conversation_markdown(self) -> str:
        """Render conversation history to a readable Markdown transcript."""
        lines = [
            "# BinAgent Conversation Transcript",
            f"Run ID: {self.context.run_id}",
            f"Mode: {self.context.mode.value}",
            f"Started: {datetime.now().isoformat()}",
            "",
        ]

        for msg in self.conversation_history:
            role = msg.role.upper()
            lines.append(f"## {role}")
            lines.append("")

            content = msg.content or ""
            if content:
                lines.append("```")
                lines.append(content)
                lines.append("```")
                lines.append("")

            if msg.tool_calls:
                lines.append("Tool Calls:")
                for tc in msg.tool_calls:
                    lines.append(f"- {tc.name}: {tc.arguments}")
                lines.append("")

            if msg.tool_results:
                lines.append("Tool Results:")
                for tr in msg.tool_results:
                    status = "ok" if tr.success else "error"
                    preview = tr.result or tr.error or ""
                    if len(preview) > 1000:
                        preview = preview[:1000] + "...[truncated]"
                    lines.append(f"- {tr.tool_name} ({status}): {preview}")
                lines.append("")

        return "\n".join(lines)

    async def get_stuck_recovery_suggestions(self) -> List[str]:
        """
        When stuck, ask LLM for alternative approaches.

        Returns:
            List of 2-3 alternative hypotheses/actions
        """
        recent_actions = []
        for msg in self.conversation_history[-5:]:
            if msg.tool_calls:
                for tc in msg.tool_calls:
                    recent_actions.append(f"- Called {tc.name}")
            if msg.tool_results:
                for tr in msg.tool_results:
                    if tr.error:
                        recent_actions.append(f"- {tr.tool_name} failed: {tr.error}")

        prompt = f"""The agent is stuck. Recent actions:
{chr(10).join(recent_actions)}

Task: {self.context.description}
Mode: {self.context.mode.value}

State explicitly WHY you are stuck, then propose 2-3 alternative hypotheses.
Format:
STUCK BECAUSE: <reason>
ALTERNATIVES:
1. <hypothesis 1>
2. <hypothesis 2>
3. <hypothesis 3>
"""

        response = await self.llm.generate(
            system_prompt="You are a security analyst helping debug a stuck analysis.",
            messages=[{"role": "user", "content": prompt}],
            tools=None,
        )

        suggestions = []
        if response.content:
            # Log the stuck analysis
            self._log("REPLAN", response.content[:500])

            for line in response.content.split('\n'):
                line = line.strip()
                if re.match(r'^\d+\.', line):
                    suggestions.append(re.sub(r'^\d+\.\s*', '', line))
                if len(suggestions) >= 3:
                    break

        return suggestions


# Export aliases for backward compatibility
TaskContext = TaskContext
