"""Main entry point for BinAgent (PentestAgent)."""

import argparse
import asyncio
import os
import sys

from ..config.constants import AGENT_MAX_ITERATIONS, DEFAULT_MODEL


def _quiet_asyncio_run(coro):
    """Run *coro* via ``asyncio.run`` while suppressing noisy SSL teardown errors.

    Python 3.10's event-loop teardown can emit ``RuntimeError: Event loop is
    closed`` on SSL transports that still have buffered writes.  This is
    harmless but ugly, so we install a custom exception handler that silences
    it during shutdown.
    """
    loop = asyncio.new_event_loop()

    def _ignore_ssl_teardown(loop, context):
        exc = context.get("exception")
        if isinstance(exc, (OSError, RuntimeError)):
            return  # suppress
        # Fall through to default for anything else
        loop.default_exception_handler(context)

    loop.set_exception_handler(_ignore_ssl_teardown)
    try:
        return loop.run_until_complete(coro)
    finally:
        try:
            _cancel_remaining(loop)
            loop.run_until_complete(loop.shutdown_asyncgens())
            loop.run_until_complete(loop.shutdown_default_executor())
        finally:
            asyncio.set_event_loop(None)
            loop.close()


def _cancel_remaining(loop):
    """Cancel all remaining tasks on the loop (mirrors asyncio.run behaviour)."""
    tasks = asyncio.all_tasks(loop)
    if not tasks:
        return
    for task in tasks:
        task.cancel()
    loop.run_until_complete(asyncio.gather(*tasks, return_exceptions=True))


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="BinAgent - AI-driven Security Analysis & CTF Solving",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Binary vulnerability analysis (uses Ghidra when available)
  binagent analyze ./binary
  binagent analyze ./binary --task "Find buffer overflow vulnerabilities"
  binagent analyze ./binary --offline    # Without Ghidra (reduced fidelity)

  # CTF solving (flags, reverse engineering, network)
  binagent solve "nc example.com 12345"
  binagent solve --file ./challenge.apk
  binagent solve --file ./binary --desc "Buffer overflow CTF"
  binagent solve --connect example.com:12345

  # Utility commands
  binagent tools list
  binagent mcp list
  binagent tui                           # Interactive mode
  binagent run -t localhost --task "scan"

Architecture:
  BinAgent uses an LLM-driven agent loop: Plan → Act → Observe → Re-plan
  Planning is MANDATORY: 3-7 validated steps before any tool calls.

Output (runs/<run-id>/):
  plan.json        - Validated execution plan
  tool_log.json    - Chronological tool calls
  evidence.json    - Evidence trail
  outcome.json     - Final results (flags/findings)
  transcript.txt   - Full execution log
        """,
    )

    parser.add_argument("--version", action="version", version="BinAgent 0.3.0")

    # Subcommands
    subparsers = parser.add_subparsers(dest="command", help="Commands")

    # Common arguments for runtime modes
    runtime_parent = argparse.ArgumentParser(add_help=False)
    runtime_parent.add_argument("--target", "-t", help="Target (IP, hostname, or URL)")
    runtime_parent.add_argument(
        "--model",
        "-m",
        default=DEFAULT_MODEL,
        help="LLM model (set PENTESTAGENT_MODEL in .env)",
    )
    runtime_parent.add_argument(
        "--docker",
        "-d",
        action="store_true",
        help="Run tools inside Docker container (requires Docker)",
    )
    runtime_parent.add_argument(
        "--playbook",
        "-p",
        help="Playbook to execute (e.g., thp3_web)",
    )

    # TUI subcommand
    subparsers.add_parser(
        "tui", parents=[runtime_parent], help="Launch TUI (Interactive Mode)"
    )

    # Run subcommand (Headless)
    run_parser = subparsers.add_parser(
        "run", parents=[runtime_parent], help="Run in headless mode"
    )
    run_parser.add_argument("task", nargs="*", help="Task to run")
    run_parser.add_argument(
        "--report",
        "-r",
        nargs="?",
        const="auto",
        help=(
            "Generate report. "
            "If used without value, auto-generates path under loot/reports/. "
            "If omitted, no report is generated."
        ),
    )
    run_parser.add_argument(
        "--max-loops",
        type=int,
        default=AGENT_MAX_ITERATIONS,
        help=f"Max agent loops before stopping (default: {AGENT_MAX_ITERATIONS})",
    )

    # ==========================================================================
    # ANALYZE subcommand - vulnerability detection / RE analysis
    # ==========================================================================
    analyze_parser = subparsers.add_parser(
        "analyze",
        help="Analyze binary/file for vulnerabilities (uses Ghidra when available)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  binagent analyze ./binary
  binagent analyze ./binary --task "Focus on format string vulnerabilities"
  binagent analyze ./app.apk
  binagent analyze ./binary --offline    # Without Ghidra MCP

Mode: ANALYZE
  - Goal: Produce security analysis report with CWE labels and evidence
  - Identifies vulnerability classes with file:line or func:offset evidence
  - Uses Ghidra MCP tools when available for deep static analysis
  - Falls back to strings/objdump/readelf when Ghidra unavailable

Output (runs/<run-id>/):
  plan.json      - Analysis plan (3-7 steps)
  tool_log.json  - All tool calls with timestamps
  evidence.json  - Evidence trail (functions, patterns)
  outcome.json   - Findings with CWE labels
  transcript.txt - Full agent execution log
        """,
    )
    analyze_parser.add_argument(
        "target_file", help="Path to the file to analyze (binary, APK, etc.)"
    )
    analyze_parser.add_argument(
        "--model",
        "-m",
        default=DEFAULT_MODEL,
        help="LLM model (set PENTESTAGENT_MODEL in .env)",
    )
    analyze_parser.add_argument(
        "--task",
        "-t",
        default="Analyze this file for security vulnerabilities. Focus on buffer overflows, format string bugs, and other memory corruption issues.",
        help="Specific analysis task/focus",
    )
    analyze_parser.add_argument(
        "--run-id",
        help="Custom run ID (default: timestamp-based)",
    )
    analyze_parser.add_argument(
        "--flag-regex",
        default=r"picoCTF\{[^}]+\}|flag\{[^}]+\}|CTF\{[^}]+\}",
        help="Regex for flags (still extracted if found during analysis)",
    )
    analyze_parser.add_argument(
        "--max-loops",
        type=int,
        default=AGENT_MAX_ITERATIONS,
        help=f"Max agent loops before stopping (default: {AGENT_MAX_ITERATIONS})",
    )
    analyze_parser.add_argument(
        "--ghidra-path",
        help="Path to Ghidra installation directory",
    )
    analyze_parser.add_argument(
        "--offline",
        action="store_true",
        help="Run without Ghidra MCP tools (reduced fidelity mode)",
    )
    analyze_parser.add_argument(
        "--previous-run",
        help="Run ID (or 'latest') of a previous analysis to build upon (skips already-found vulnerabilities)",
    )
    analyze_parser.add_argument(
        "--doublerun",
        action="store_true",
        help="Automated two-pass analysis: Run A for coverage, Run B for depth on new paths",
    )

    # ==========================================================================
    # SOLVE subcommand - CTF-style flag finding
    # ==========================================================================
    solve_parser = subparsers.add_parser(
        "solve",
        help="Solve CTF challenges (files, network, reverse engineering)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Network/netcat challenges
  binagent solve "nc example.com 12345"
  binagent solve --connect example.com:12345 --desc "SQL injection"

  # File-based challenges (binary, APK, archive)
  binagent solve --file ./challenge.apk
  binagent solve --file ./binary --desc "Buffer overflow CTF"

  # Combined
  binagent solve --file ./exploit.py --connect target:1337

Mode: SOLVE
  - Goal: Obtain the flag and provide evidence trail
  - Treats description as puzzle specification
  - Reverse engineers files (binary, APK, archive)
  - Interacts with network services (netcat)
  - Decodes/transforms content (base64, hex, XOR, etc.)

Output (runs/<run-id>/):
  plan.json      - Solving strategy (3-7 steps)
  tool_log.json  - All tool calls with timestamps
  evidence.json  - Evidence chain to flag
  outcome.json   - Found flags
  transcript.txt - Full agent execution log
        """,
    )
    solve_parser.add_argument(
        "description",
        nargs="*",
        help="Challenge description (can include 'nc host port')",
    )
    solve_parser.add_argument(
        "--file", "-f",
        help="Challenge file (APK, binary, archive, etc.)",
    )
    solve_parser.add_argument(
        "--connect", "-c",
        help="Target host:port (e.g., example.com:12345)",
    )
    solve_parser.add_argument(
        "--desc", "-d",
        help="Additional description text",
    )
    solve_parser.add_argument(
        "--model", "-m",
        default=DEFAULT_MODEL,
        help="LLM model (set PENTESTAGENT_MODEL in .env)",
    )
    solve_parser.add_argument(
        "--run-id",
        help="Custom run ID (default: timestamp-based)",
    )
    solve_parser.add_argument(
        "--flag-regex",
        default=r"picoCTF\{[^}]+\}|flag\{[^}]+\}|CTF\{[^}]+\}",
        help="Regex to match flags",
    )
    solve_parser.add_argument(
        "--max-loops",
        type=int,
        default=AGENT_MAX_ITERATIONS,
        help=f"Max agent loops before stopping (default: {AGENT_MAX_ITERATIONS})",
    )
    solve_parser.add_argument(
        "--offline",
        action="store_true",
        help="Run without Ghidra MCP tools",
    )

    # ==========================================================================
    # Tools subcommand
    # ==========================================================================
    tools_parser = subparsers.add_parser("tools", help="Manage tools")
    tools_subparsers = tools_parser.add_subparsers(
        dest="tools_command", help="Tool commands"
    )

    # tools list
    tools_subparsers.add_parser("list", help="List all available tools")

    # tools info
    tools_info = tools_subparsers.add_parser("info", help="Show tool details")
    tools_info.add_argument("name", help="Tool name")

    # tools env
    tools_subparsers.add_parser("env", help="Show detected CLI tools in environment")

    # ==========================================================================
    # MCP subcommand
    # ==========================================================================
    mcp_parser = subparsers.add_parser("mcp", help="Manage MCP servers")
    mcp_subparsers = mcp_parser.add_subparsers(dest="mcp_command", help="MCP commands")

    # mcp list
    mcp_subparsers.add_parser("list", help="List configured MCP servers")

    # mcp add
    mcp_add = mcp_subparsers.add_parser("add", help="Add an MCP server")
    mcp_add.add_argument("name", help="Server name")
    mcp_add.add_argument("command", help="Command to run (e.g., npx)")
    mcp_add.add_argument("args", nargs="*", help="Command arguments")
    mcp_add.add_argument("--description", default="", help="Server description")

    # mcp remove
    mcp_remove = mcp_subparsers.add_parser("remove", help="Remove an MCP server")
    mcp_remove.add_argument("name", help="Server name to remove")

    # mcp disable
    mcp_disable = mcp_subparsers.add_parser(
        "disable", help="Disable an MCP server (update config)"
    )
    mcp_disable.add_argument("name", help="Server name to disable")

    # mcp enable
    mcp_enable = mcp_subparsers.add_parser(
        "enable", help="Enable an MCP server (update config)"
    )
    mcp_enable.add_argument("name", help="Server name to enable")

    # mcp test
    mcp_test = mcp_subparsers.add_parser("test", help="Test MCP server connection")
    mcp_test.add_argument("name", help="Server name to test")

    return parser, parser.parse_args()


def handle_tools_command(args: argparse.Namespace):
    """Handle tools subcommand."""
    from rich.console import Console
    from rich.table import Table

    from ..tools import get_all_tools, get_tool

    console = Console()

    if args.tools_command == "list":
        tools = get_all_tools()

        if not tools:
            console.print("[yellow]No tools found[/]")
            return

        table = Table(title="Available Tools")
        table.add_column("Name", style="cyan")
        table.add_column("Category", style="green")
        table.add_column("Description")

        for tool in sorted(tools, key=lambda t: t.name):
            desc = (
                tool.description[:50] + "..."
                if len(tool.description) > 50
                else tool.description
            )
            table.add_row(tool.name, tool.category, desc)

        console.print(table)
        console.print(f"\nTotal: {len(tools)} tools")

    elif args.tools_command == "info":
        tool = get_tool(args.name)
        if not tool:
            console.print(f"[red]Tool not found: {args.name}[/]")
            return

        console.print(f"\n[bold cyan]{tool.name}[/]")
        console.print(f"[dim]Category:[/] {tool.category}")
        console.print(f"\n{tool.description}")

        if tool.schema.properties:
            console.print("\n[bold]Parameters:[/]")
            for name, props in tool.schema.properties.items():
                required = (
                    "required" if name in (tool.schema.required or []) else "optional"
                )
                ptype = props.get("type", "any")
                desc = props.get("description", "")
                console.print(f"  [cyan]{name}[/] ({ptype}, {required}): {desc}")

    elif args.tools_command == "env":
        from ..runtime.runtime import detect_environment

        env = detect_environment()

        console.print("\n[bold]Environment:[/]")
        console.print(f"  OS: {env.os} ({env.os_version})")
        console.print(f"  Architecture: {env.architecture}")
        console.print(f"  Shell: {env.shell}")

        if env.available_tools:
            console.print(
                f"\n[bold]Detected CLI Tools ({len(env.available_tools)}):[/]"
            )

            # Group by category
            by_category = {}
            for tool_info in env.available_tools:
                if tool_info.category not in by_category:
                    by_category[tool_info.category] = []
                by_category[tool_info.category].append(tool_info)

            for category in sorted(by_category.keys()):
                tools_in_cat = by_category[category]
                console.print(f"\n[bold cyan]{category}[/] ({len(tools_in_cat)}):")
                for tool_info in sorted(tools_in_cat, key=lambda t: t.name):
                    console.print(f"  • {tool_info.name}")
        else:
            console.print("\n[yellow]No CLI tools detected[/]")
            console.print(
                "\n[dim]Tip: Install tools like nmap, curl, git to expand capabilities[/]"
            )

    else:
        console.print("[yellow]Use 'binagent tools --help' for commands[/]")


def handle_mcp_command(args: argparse.Namespace):
    """Handle MCP subcommand."""
    from rich.console import Console
    from rich.table import Table

    from ..mcp.manager import MCPManager

    console = Console()
    manager = MCPManager()

    if args.mcp_command == "list":
        servers = manager.list_configured_servers()

        if not servers:
            console.print("[yellow]No MCP servers configured[/]")
            console.print(
                "\nAdd a server with: binagent mcp add <name> <command> <args...>"
            )
            return

        table = Table(title="Configured MCP Servers")
        table.add_column("Name", style="cyan")
        table.add_column("Command", style="green")
        table.add_column("Args")
        table.add_column("Connected", style="yellow")

        for server in servers:
            args_str = " ".join(server["args"][:3])
            if len(server["args"]) > 3:
                args_str += "..."
            connected = "+" if server.get("connected") else "-"
            table.add_row(server["name"], server["command"], args_str, connected)

        console.print(table)
        console.print(f"\nConfig file: {manager.config_path}")

    elif args.mcp_command == "add":
        manager.add_server(
            name=args.name,
            command=args.command,
            args=args.args or [],
            description=args.description,
        )
        console.print(f"[green]Added MCP server: {args.name}[/]")
        console.print(f"  Command: {args.command} {' '.join(args.args or [])}")

    elif args.mcp_command == "remove":
        if manager.remove_server(args.name):
            console.print(f"[yellow]Removed MCP server: {args.name}[/]")
        else:
            console.print(f"[red]Server not found: {args.name}[/]")

    elif args.mcp_command == "disable":
        if manager.set_enabled(args.name, False):
            console.print(f"[yellow]Disabled MCP server in config: {args.name}[/]")
        else:
            console.print(f"[red]Server not found: {args.name}[/]")

    elif args.mcp_command == "enable":
        if manager.set_enabled(args.name, True):
            console.print(f"[green]Enabled MCP server in config: {args.name}[/]")
        else:
            console.print(f"[red]Server not found: {args.name}[/]")

    elif args.mcp_command == "test":
        console.print(f"[bold]Testing MCP server: {args.name}[/]\n")

        async def test_server():
            server = await manager.connect_server(args.name)
            if server and server.connected:
                console.print("[green]+ Connected successfully![/]")
                console.print(f"\n[bold]Available tools ({len(server.tools)}):[/]")
                for tool in server.tools:
                    desc = tool.get("description", "No description")[:60]
                    console.print(f"  [cyan]{tool['name']}[/]: {desc}")
                await manager.disconnect_all()
            else:
                console.print("[red]x Failed to connect[/]")

        asyncio.run(test_server())

    else:
        console.print("[yellow]Use 'binagent mcp --help' for available commands[/]")


def _render_setup_panel(console, data: dict):
    """Render the Stage 0 Run Setup Rich panel from setup verification data."""
    from rich.panel import Panel as _P
    from rich.text import Text as _T

    d = data
    st = d.get("shell_tools", {})
    t = _T()

    # ── 1) Inputs ────────────────────────────────────────────────────
    t.append("Inputs\n", "bold underline")
    t.append(f"  Binary:        {d['binary_path']}\n", "cyan")
    t.append(f"  Mode:          {d.get('mode', 'ANALYZE')}\n")
    t.append(f"  Model:         {d['llm_model']}\n")
    t.append(f"  Max loops:     {d['max_iterations']}   Timeout: {d.get('timeout') or 'none'}\n")
    prev = d.get("previous_run", "none")
    loaded = "yes" if d.get("previous_run_loaded") else "no"
    t.append(f"  Previous run:  {prev}   Loaded: {loaded}\n")
    t.append(f"  Artifacts dir: {d.get('artifacts_dir') or 'none'}\n")

    # ── 2) Binary fingerprint ────────────────────────────────────────
    t.append("\n")
    t.append("Binary fingerprint\n", "bold underline")
    sha = d.get("sha256", "N/A")
    sha_display = sha[:12] if sha and sha != "N/A" else "N/A"
    t.append(f"  sha256:      {sha_display}\n", "dim")
    t.append(f"  size_bytes:  {d.get('size_bytes', 0)}\n", "dim")
    t.append(f"  file type:   {d.get('file_type', 'N/A')}\n", "dim")

    # ── 3) Tool availability ─────────────────────────────────────────
    t.append("\n")
    t.append("Tool availability\n", "bold underline")
    for tool_name in ["file", "readelf", "objdump", "strings"]:
        status = st.get(tool_name, "?")
        style = "green" if status == "OK" else "red"
        t.append(f"  {tool_name:12s} ", "dim")
        t.append(f"{status}\n", style)
    checksec_st = st.get("checksec", "MISSING")
    cs_style = "green" if checksec_st == "OK" else "yellow" if checksec_st == "MISSING" else "dim"
    t.append(f"  {'checksec':12s} ", "dim")
    t.append(f"{checksec_st}\n", cs_style)

    # ── 4) Ghidra MCP backend ────────────────────────────────────────
    t.append("\n")
    t.append("Ghidra MCP backend\n", "bold underline")
    ghidra = "CONNECTED" if d.get("ghidra_connected") else "FAILED"
    ghidra_style = "green" if d.get("ghidra_connected") else "red bold"
    t.append("  MCP ghidra-local:  ")
    t.append(f"{ghidra}\n", ghidra_style)
    t.append(f"  Ghidra version:    {d.get('ghidra_version', 'N/A')}\n", "dim")
    t.append(f"  MCP server ver:    {d.get('ghidra_mcp_version', 'N/A')}\n", "dim")
    t.append(f"  Project/workspace: {d.get('ghidra_project', 'N/A')}\n", "dim")
    t.append(f"  Program name:      {d.get('ghidra_program_name') or 'none'}\n")
    imported = "yes" if d.get("ghidra_imported_this_run") else "no"
    t.append(f"  imported_this_run: {imported}\n", "dim")
    ac = d.get("ghidra_analysis_complete", "N/A")
    cc = d.get("ghidra_code_collection", "N/A")
    sc = d.get("ghidra_strings_collection", "N/A")
    t.append(f"  Flags: analysis_complete={ac}  code_collection={cc}  strings_collection={sc}\n", "dim")

    # ── 5) Framework provenance ──────────────────────────────────────
    t.append("\n")
    t.append("Framework provenance\n", "bold underline")
    commit = d.get("git_commit", "unknown")
    dirty = "yes" if d.get("git_dirty") else "no"
    t.append(f"  BinAgent commit:   {commit} (dirty={dirty})\n", "dim")
    t.append(f"  LLM provider:      {d.get('llm_provider', 'LiteLLM')}\n", "dim")
    t.append(f"  Model:             {d['llm_model']}\n", "dim")

    # ── Multi-pass merge (only when previous-run used) ───────────────
    merge = d.get("merge_policy")
    if merge:
        t.append("\n")
        t.append("Multi-pass merge\n", "bold underline")
        artifacts = ", ".join(merge.get("merged_artifacts", []))
        t.append(f"  merged_artifacts:  [{artifacts}]\n", "dim")
        t.append(f"  added_findings:    {merge.get('added_findings', 0)}\n", "dim")

    console.print(_P(t, title="[bold]Run Setup", border_style="cyan"))
    if not d.get("ok"):
        console.print(f"[red bold]SETUP FAILED: {d.get('stop_reason')}[/]")


def handle_analyze_command(args: argparse.Namespace):
    """
    Handle ANALYZE command using GeneralAgent with AgentMode.ANALYZE.

    Mode: vulnerability detection / RE analysis
    Goal: Produce security analysis report with CWE labels and evidence
    """
    from rich.console import Console
    from rich.panel import Panel
    from rich.text import Text

    from ..agents import GeneralAgent, TaskContext, AgentMode
    from ..tools.general import register_general_tools

    console = Console()

    # Validate file exists
    target_file = args.target_file
    if not os.path.isfile(target_file):
        console.print(f"[red]Error: File not found: {target_file}[/]")
        return

    target_file = os.path.abspath(target_file)

    # Auto-scale max-loops based on file size (only when user didn't override)
    _user_set_max_loops = "--max-loops" in sys.argv
    _is_doublerun = getattr(args, "doublerun", False)
    if not _user_set_max_loops:
        file_size_mb = os.path.getsize(target_file) / (1024 * 1024)
        if _is_doublerun:
            # Doublerun needs more budget — two passes share the total
            if file_size_mb > 2.5:
                args.max_loops = max(args.max_loops, 45)
            elif file_size_mb >= 1.0:
                args.max_loops = max(args.max_loops, 35)
            else:
                args.max_loops = max(args.max_loops, 25)
        else:
            if file_size_mb > 2.5:
                args.max_loops = max(args.max_loops, 25)
            elif file_size_mb >= 1.0:
                args.max_loops = max(args.max_loops, 20)
            else:
                args.max_loops = max(args.max_loops, 12)

    # Load previous run findings if specified
    previous_findings = []
    prev_run_id = None
    if getattr(args, "previous_run", None):
        import json as _json
        prev_run_id = args.previous_run

        # Support "latest" — find most recent run for the same binary
        if prev_run_id == "latest":
            prev_run_id = None
            runs_dir = "runs"
            if os.path.isdir(runs_dir):
                candidates = []
                for d in sorted(os.listdir(runs_dir), reverse=True):
                    rj = os.path.join(runs_dir, d, "run.json")
                    if not os.path.isfile(rj):
                        continue
                    try:
                        with open(rj) as f:
                            meta = _json.load(f)
                        ctx = meta.get("context", {})
                        if ctx.get("mode") == "analyze" and ctx.get("file_path") == target_file:
                            candidates.append(d)
                    except (ValueError, OSError):
                        continue
                if candidates:
                    prev_run_id = candidates[0]  # most recent (sorted descending)
                    console.print(f"[dim]Resolved --previous-run latest → {prev_run_id}[/]")
                else:
                    console.print(f"[yellow]Warning: No previous analyze run found for {os.path.basename(target_file)}[/]")

        if prev_run_id:
            _prev_path = os.path.join("runs", prev_run_id, "outcome.json")
            if os.path.isfile(_prev_path):
                with open(_prev_path) as f:
                    _prev = _json.load(f)
                previous_findings = _prev.get("findings", [])
                console.print(f"[dim]Loaded {len(previous_findings)} findings from previous run {prev_run_id}[/]")
            else:
                console.print(f"[yellow]Warning: Previous run not found: {_prev_path}[/]")

    # Set Ghidra path if provided
    if args.ghidra_path:
        os.environ["GHIDRA_INSTALL_DIR"] = args.ghidra_path

    # Check model
    model = getattr(args, 'model', None) or DEFAULT_MODEL
    if not model:
        console.print("[red]Error: No model configured.[/]")
        console.print("Set PENTESTAGENT_MODEL in .env or use --model flag.")
        return

    # Build task context
    from datetime import datetime
    context = TaskContext(
        mode=AgentMode.ANALYZE,
        description=args.task,
        file_path=target_file,
        flag_regex=args.flag_regex,
        run_id=args.run_id or datetime.now().strftime("%Y%m%d_%H%M%S"),
        previous_findings=previous_findings or None,
        previous_run_id=prev_run_id,
    )

    # Display startup info
    start_text = Text()
    start_text.append("BINAGENT ANALYZE\n\n", style="bold white")
    start_text.append("File:    ", style="dim")
    start_text.append(f"{target_file}\n", style="cyan")
    start_text.append("Task:    ", style="dim")
    task_preview = args.task[:60] + "..." if len(args.task) > 60 else args.task
    start_text.append(f"{task_preview}\n", style="white")
    start_text.append("Model:   ", style="dim")
    start_text.append(f"{model}\n", style="white")
    start_text.append("Loops:   ", style="dim")
    start_text.append(f"{args.max_loops}\n", style="white")
    if args.offline:
        start_text.append("Ghidra:  ", style="dim")
        start_text.append("DISABLED (offline mode)\n", style="yellow")
    if previous_findings:
        start_text.append("Prior:   ", style="dim")
        start_text.append(f"{len(previous_findings)} findings from {prev_run_id}\n", style="yellow")

    console.print()
    console.print(Panel(start_text, title="[dim]Starting Analysis", border_style="dim"))
    console.print()

    # Live display callback
    import time as _time
    _start_time = _time.time()
    _tool_count = 0

    def _print_status(msg: str, style: str = "dim"):
        elapsed = int(_time.time() - _start_time)
        mins, secs = divmod(elapsed, 60)
        ts = f"[{mins:02d}:{secs:02d}]"
        console.print(f"[dim]{ts}[/] [{style}]{msg}[/]")

    def _on_message(phase: str, data: dict):
        nonlocal _tool_count
        if phase == "setup":
            _render_setup_panel(console, data)

        elif phase == "preflight":
            status = data.get("status", "")
            if status == "starting":
                _print_status("Preflight: gathering initial evidence...", "cyan")
            elif status == "ghidra_waiting":
                elapsed = data.get("elapsed", 0)
                _print_status(f"Waiting for Ghidra analysis to complete... ({elapsed}s)", "cyan")
            elif status == "done":
                n = data.get("tool_calls", 0)
                _print_status(f"Preflight complete ({n} tool calls)", "green")
            elif status == "error":
                _print_status("Preflight encountered errors (continuing)", "yellow")

        elif phase == "plan":
            status = data.get("status", "")
            if status == "generating":
                _print_status("Generating analysis plan...", "cyan")
            elif status == "ready":
                steps = data.get("steps", [])
                fallback = data.get("fallback", False)
                label = " (fallback)" if fallback else ""
                console.print()
                _print_status(f"Plan ready{label} ({len(steps)} steps):", "green")
                for s in steps:
                    console.print(f"           [dim]{s['number']}.[/] {s['description'][:90]}")
                console.print()

        elif phase == "thinking":
            content = data.get("content", "")
            if content:
                from rich.panel import Panel
                from rich.markdown import Markdown
                console.print()
                console.print(Panel(
                    Markdown(content),
                    title="[dim]Agent",
                    border_style="dim",
                ))

        elif phase == "tool_call":
            _tool_count += 1
            name = data.get("name", "?")
            arguments = data.get("arguments", {})
            cmd = arguments.get("command", "")
            console.print()
            _print_status(f"$ {name} ({_tool_count})", "#7a7a7a")
            if cmd:
                display_cmd = cmd[:100] + ("..." if len(cmd) > 100 else "")
                console.print(f"           [dim]{display_cmd}[/]")

        elif phase == "tool_result":
            name = data.get("name", "?")
            success = data.get("success", True)
            error = data.get("error")
            result = data.get("result", "")
            if error:
                console.print(f"           [dim][!] {error[:120]}[/]")
            elif result:
                preview = result[:100].replace("\n", " ")
                if "success" in preview.lower() or not preview.strip():
                    console.print(f"           [dim][+] OK[/]")
                else:
                    console.print(f"           [dim][*] {preview[:80]}...[/]")
            else:
                console.print(f"           [dim][+] OK[/]")

    # Shared setup for agent runs
    async def _setup_tools():
        from ..llm import LLM
        from ..runtime import LocalRuntime
        from ..tools import get_all_tools
        from ..mcp.manager import MCPManager

        llm = LLM(model=model)
        runtime = LocalRuntime()
        register_general_tools()
        tools = list(get_all_tools())

        mcp_manager = None
        if not args.offline:
            console.print("[dim]Connecting MCP servers...[/]")
            mcp_manager = MCPManager()
            mcp_tools = await mcp_manager.connect_all()
            tools.extend(mcp_tools)
            console.print(f"[dim]MCP tools added: {len(mcp_tools)}[/]")

        console.print(f"[dim]Total tools available: {len(tools)}[/]")
        return llm, runtime, tools, mcp_manager

    # ------------------------------------------------------------------
    # Double-run mode
    # ------------------------------------------------------------------
    if getattr(args, "doublerun", False):
        from datetime import datetime as _dt
        base_run_id = args.run_id or _dt.now().strftime("%Y%m%d_%H%M%S")

        # Dynamic budget: Run A gets a generous cap (2/3), Run B gets remainder
        total_loops = args.max_loops
        run_a_cap = max(total_loops * 2 // 3, 10)
        run_b_min = max(total_loops // 3, 6)

        console.print(f"[dim]Double-run mode: Run A (up to {run_a_cap} loops) + Run B (at least {run_b_min} loops)[/]")
        console.print()

        async def run_doublerun():
            import json as _json
            llm, runtime, tools, mcp_manager = await _setup_tools()
            console.print(f"[dim]LLM-driven agent loop starting (doublerun)...[/]")
            console.print()

            # --- RUN A (coverage-first) ---
            console.print("[bold cyan]═══ RUN A (breadth/coverage) ═══[/]")
            console.print()
            context_a = TaskContext(
                mode=AgentMode.ANALYZE,
                description=args.task,
                file_path=target_file,
                flag_regex=args.flag_regex,
                run_id=base_run_id + "_runA",
                doublerun_phase="A",
                previous_findings=previous_findings or None,
                previous_run_id=prev_run_id,
            )
            agent_a = GeneralAgent(
                llm=llm, tools=tools, runtime=runtime,
                context=context_a, max_iterations=run_a_cap,
            )
            result_a = await agent_a.solve(on_message=_on_message)
            if result_a.get("setup_failed"):
                if mcp_manager:
                    await mcp_manager.disconnect_all()
                return result_a
            run_a_context = agent_a.build_run_a_context()

            # Show Run A summary
            n_a = len(result_a.get("findings", []))
            cwes_a = sorted(set(f.get("cwe", "?") for f in result_a.get("findings", [])))
            console.print()
            console.print(f"[dim]Run A complete: {n_a} findings ({', '.join(cwes_a) or 'none'})[/]")
            console.print()

            # --- RUN B (steering/depth) ---
            # Dynamically allocate remaining budget to Run B
            run_a_used = getattr(agent_a, '_execution_iterations', run_a_cap)
            run_b_budget = max(total_loops - run_a_used, run_b_min)
            console.print(f"[dim]Run A used {run_a_used} loops → Run B gets {run_b_budget} loops[/]")

            console.print("[bold cyan]═══ RUN B (depth/new paths) ═══[/]")
            console.print()

            # Capture only PSEUDOCODE_CHUNK messages from Run A's preflight.
            # The full conversation_history has ~160+ messages (tool calls,
            # intermediate results, Run A analysis) which overflows LLM context
            # when combined with Run B's own tool calls.  Run B only needs the
            # decompiled pseudocode to have code context for exploration.
            preflight_hist = [
                msg for msg in agent_a.conversation_history
                if getattr(msg, "metadata", None)
                and msg.metadata.get("preflight_decompile")
            ]

            context_b = TaskContext(
                mode=AgentMode.ANALYZE,
                description=args.task,
                file_path=target_file,
                flag_regex=args.flag_regex,
                run_id=base_run_id + "_runB",
                doublerun_phase="B",
                run_a_context=run_a_context,
                previous_findings=result_a.get("findings", []),
                preflight_history=preflight_hist,
            )
            agent_b = GeneralAgent(
                llm=llm, tools=tools, runtime=runtime,
                context=context_b, max_iterations=run_b_budget,
            )
            # Transfer Ghidra state so Run B can use Ghidra without re-import
            agent_b._ghidra_binary_name = agent_a._ghidra_binary_name

            result_b = await agent_b.solve(on_message=_on_message)

            # Disconnect MCP cleanly
            if mcp_manager:
                await mcp_manager.disconnect_all()
                await asyncio.sleep(0.2)

            # Merge findings (deduplicate by cwe+function)
            all_findings = list(result_a.get("findings", []))
            seen = set()
            for f in all_findings:
                key = (f.get("cwe", ""), f.get("function", ""), f.get("evidence", "")[:80])
                seen.add(key)
            for f in result_b.get("findings", []):
                key = (f.get("cwe", ""), f.get("function", ""), f.get("evidence", "")[:80])
                if key not in seen:
                    all_findings.append(f)
                    seen.add(key)

            # Save merged outcome
            merged_dir = os.path.join("runs", base_run_id)
            os.makedirs(merged_dir, exist_ok=True)
            merged_outcome = {
                "mode": "analyze",
                "doublerun": True,
                "success": len(all_findings) > 0,
                "findings": all_findings,
                "run_a_id": base_run_id + "_runA",
                "run_b_id": base_run_id + "_runB",
                "run_a_findings_count": len(result_a.get("findings", [])),
                "run_b_findings_count": len(result_b.get("findings", [])),
                "elapsed_seconds": result_a["elapsed_seconds"] + result_b["elapsed_seconds"],
            }
            with open(os.path.join(merged_dir, "outcome.json"), "w") as f:
                _json.dump(merged_outcome, f, indent=2)

            return {
                "success": len(all_findings) > 0,
                "findings": all_findings,
                "flags": list(set(result_a.get("flags", []) + result_b.get("flags", []))),
                "run_id": base_run_id,
                "out_dir": merged_dir,
                "elapsed_seconds": result_a["elapsed_seconds"] + result_b["elapsed_seconds"],
                "ghidra_available": result_a.get("ghidra_available", False),
                "used_fallback_plan": result_a.get("used_fallback_plan", False),
                "run_a": result_a,
                "run_b": result_b,
            }

        try:
            result = _quiet_asyncio_run(run_doublerun())
        except KeyboardInterrupt:
            console.print("\n[yellow]Interrupted by user[/]")
            return
        except Exception as e:
            console.print(f"[red]Error: {e}[/]")
            import traceback
            traceback.print_exc()
            return

        # Display doublerun results
        console.print()
        console.print("=" * 60)
        if result["success"]:
            console.print("[bold green]ANALYSIS COMPLETE - DOUBLERUN[/]")
        else:
            console.print("[bold yellow]ANALYSIS COMPLETE - DOUBLERUN (NO FINDINGS)[/]")
        console.print("=" * 60)

        result_a = result["run_a"]
        result_b = result["run_b"]
        cwes_a = sorted(set(f.get("cwe", "?") for f in result_a.get("findings", [])))
        cwes_b = sorted(set(f.get("cwe", "?") for f in result_b.get("findings", [])))
        cwes_all = sorted(set(f.get("cwe", "?") for f in result.get("findings", [])))

        console.print(f"Run A:        {len(result_a.get('findings', []))} findings ({', '.join(cwes_a) or 'none'})")
        console.print(f"Run B:        {len(result_b.get('findings', []))} findings ({', '.join(cwes_b) or 'none'})")
        console.print(f"Total unique: {len(result['findings'])} findings ({', '.join(cwes_all) or 'none'})")
        console.print(f"Elapsed:      {result['elapsed_seconds']:.2f}s")
        console.print(f"Ghidra:       {'Available' if result.get('ghidra_available') else 'Not available'}")

        if result.get("findings"):
            console.print(f"\n[bold]All findings ({len(result['findings'])}):[/]")
            for f in result["findings"][:15]:
                cwe = f.get("cwe", "Unknown")
                func = f.get("function", "")[:30]
                console.print(f"  [yellow]{cwe}[/] {func}")

        console.print(f"\n[dim]Artifacts:[/]")
        console.print(f"  {result_a['out_dir']}/  (Run A)")
        console.print(f"  {result_b['out_dir']}/  (Run B)")
        console.print(f"  {result['out_dir']}/outcome.json  (merged)")
        console.print("=" * 60)
        return

    # ------------------------------------------------------------------
    # Single-run mode (default)
    # ------------------------------------------------------------------
    async def run_agent():
        llm, runtime, tools, mcp_manager = await _setup_tools()
        console.print(f"[dim]LLM-driven agent loop starting...[/]")
        console.print()

        agent = GeneralAgent(
            llm=llm,
            tools=tools,
            runtime=runtime,
            context=context,
            max_iterations=args.max_loops,
        )

        result = await agent.solve(on_message=_on_message)

        # Disconnect MCP cleanly (allow SSL transports to flush)
        if mcp_manager:
            await mcp_manager.disconnect_all()
            await asyncio.sleep(0.2)

        return result

    try:
        result = _quiet_asyncio_run(run_agent())
    except KeyboardInterrupt:
        console.print("\n[yellow]Interrupted by user[/]")
        return
    except Exception as e:
        console.print(f"[red]Error: {e}[/]")
        import traceback
        traceback.print_exc()
        return

    # Display results
    console.print()
    console.print("=" * 60)
    if result["success"]:
        console.print("[bold green]ANALYSIS COMPLETE - FINDINGS IDENTIFIED[/]")
    else:
        console.print("[bold yellow]ANALYSIS COMPLETE - NO FINDINGS[/]")
    console.print("=" * 60)

    console.print(f"Run ID:       {result['run_id']}")
    console.print(f"Elapsed:      {result['elapsed_seconds']:.2f}s")
    console.print(f"Ghidra:       {'Available' if result.get('ghidra_available') else 'Not available'}")
    console.print(f"Fallback:     {'Yes' if result.get('used_fallback_plan') else 'No'}")

    if result.get("findings"):
        console.print(f"\n[bold]Findings ({len(result['findings'])}):[/]")
        for f in result["findings"][:10]:
            cwe = f.get("cwe", "Unknown")
            ctx = f.get("context", "")[:80]
            console.print(f"  [yellow]{cwe}[/]: {ctx}...")

    if result.get("flags"):
        console.print(f"\n[bold green]Flags found ({len(result['flags'])}):[/]")
        for flag in result["flags"]:
            console.print(f"  [bold green]{flag}[/]")

    console.print(f"\n[dim]Artifacts: {result['out_dir']}/[/]")
    console.print(f"  plan.json      - Analysis plan")
    console.print(f"  tool_log.json  - Tool call history")
    console.print(f"  evidence.json  - Evidence trail")
    console.print(f"  outcome.json   - Findings with CWE")
    console.print(f"  transcript.txt - Full execution log")
    console.print("=" * 60)


def handle_solve_command(args: argparse.Namespace):
    """
    Handle SOLVE command using GeneralAgent with AgentMode.SOLVE.

    Mode: CTF-style flag finding
    Goal: Obtain the flag and provide evidence trail
    """
    from rich.console import Console
    from rich.panel import Panel
    from rich.text import Text

    from ..agents import GeneralAgent, TaskContext, AgentMode
    from ..ctf import parse_nc_target, parse_connect_string
    from ..tools.general import register_general_tools

    console = Console()

    # Collect description from various sources
    description_parts = []
    if args.description:
        description_parts.append(" ".join(args.description))
    if args.desc:
        description_parts.append(args.desc)
    full_description = " ".join(description_parts)

    # Parse network target
    host = None
    port = None

    # Try --connect first
    if args.connect:
        parsed = parse_connect_string(args.connect)
        if parsed:
            host = parsed.host
            port = parsed.port
        else:
            console.print(f"[red]Error: Invalid --connect format: {args.connect}[/]")
            console.print("Expected format: host:port")
            return

    # Try to extract from description if not provided
    if not host and full_description:
        parsed = parse_nc_target(full_description)
        if parsed:
            host = parsed.host
            port = parsed.port
            console.print(f"[dim]Extracted target: {host}:{port}[/]")

    # Validate file if provided
    file_path = None
    if args.file:
        if not os.path.isfile(args.file):
            console.print(f"[red]Error: File not found: {args.file}[/]")
            return
        file_path = os.path.abspath(args.file)

    # Need at least description, file, or host:port
    if not full_description and not file_path and not (host and port):
        console.print("[red]Error: Provide description, --file, or --connect[/]")
        console.print("\nExamples:")
        console.print('  binagent solve "nc example.com 12345"')
        console.print("  binagent solve --file ./challenge.apk")
        console.print("  binagent solve --connect example.com:12345")
        return

    # Check model
    model = getattr(args, 'model', None) or DEFAULT_MODEL
    if not model:
        console.print("[red]Error: No model configured.[/]")
        console.print("Set PENTESTAGENT_MODEL in .env or use --model flag.")
        return

    # Build task context
    from datetime import datetime
    context = TaskContext(
        mode=AgentMode.SOLVE,
        description=full_description,
        file_path=file_path,
        host=host,
        port=port,
        flag_regex=args.flag_regex,
        run_id=args.run_id or datetime.now().strftime("%Y%m%d_%H%M%S"),
    )

    # Display startup info
    start_text = Text()
    start_text.append("BINAGENT SOLVE\n\n", style="bold white")
    if full_description:
        desc_preview = full_description[:80] + "..." if len(full_description) > 80 else full_description
        start_text.append("Description: ", style="dim")
        start_text.append(f"{desc_preview}\n", style="white")
    if file_path:
        start_text.append("File:        ", style="dim")
        start_text.append(f"{file_path}\n", style="cyan")
    if host and port:
        start_text.append("Target:      ", style="dim")
        start_text.append(f"{host}:{port}\n", style="cyan")
    start_text.append("Model:       ", style="dim")
    start_text.append(f"{model}\n", style="white")

    console.print()
    console.print(Panel(start_text, title="[dim]Starting Solver", border_style="dim"))
    console.print()

    # Live display callback
    import time as _time
    _solve_start = _time.time()
    _solve_tool_count = 0

    def _solve_print_status(msg: str, style: str = "dim"):
        elapsed = int(_time.time() - _solve_start)
        mins, secs = divmod(elapsed, 60)
        ts = f"[{mins:02d}:{secs:02d}]"
        console.print(f"[dim]{ts}[/] [{style}]{msg}[/]")

    def _solve_on_message(phase: str, data: dict):
        nonlocal _solve_tool_count
        if phase == "setup":
            _render_setup_panel(console, data)

        elif phase == "preflight":
            status = data.get("status", "")
            if status == "starting":
                _solve_print_status("Preflight: gathering initial evidence...", "cyan")
            elif status == "ghidra_waiting":
                elapsed = data.get("elapsed", 0)
                _solve_print_status(f"Waiting for Ghidra analysis to complete... ({elapsed}s)", "cyan")
            elif status == "done":
                n = data.get("tool_calls", 0)
                _solve_print_status(f"Preflight complete ({n} tool calls)", "green")
            elif status == "error":
                _solve_print_status("Preflight encountered errors (continuing)", "yellow")

        elif phase == "plan":
            status = data.get("status", "")
            if status == "generating":
                _solve_print_status("Generating solving plan...", "cyan")
            elif status == "ready":
                steps = data.get("steps", [])
                fallback = data.get("fallback", False)
                label = " (fallback)" if fallback else ""
                console.print()
                _solve_print_status(f"Plan ready{label} ({len(steps)} steps):", "green")
                for s in steps:
                    console.print(f"           [dim]{s['number']}.[/] {s['description'][:90]}")
                console.print()

        elif phase == "thinking":
            content = data.get("content", "")
            if content:
                from rich.panel import Panel
                from rich.markdown import Markdown
                console.print()
                console.print(Panel(
                    Markdown(content),
                    title="[dim]Agent",
                    border_style="dim",
                ))

        elif phase == "tool_call":
            _solve_tool_count += 1
            name = data.get("name", "?")
            arguments = data.get("arguments", {})
            cmd = arguments.get("command", "")
            console.print()
            _solve_print_status(f"$ {name} ({_solve_tool_count})", "#7a7a7a")
            if cmd:
                display_cmd = cmd[:100] + ("..." if len(cmd) > 100 else "")
                console.print(f"           [dim]{display_cmd}[/]")

        elif phase == "tool_result":
            name = data.get("name", "?")
            error = data.get("error")
            result = data.get("result", "")
            if error:
                console.print(f"           [dim][!] {error[:120]}[/]")
            elif result:
                preview = result[:100].replace("\n", " ")
                if "success" in preview.lower() or not preview.strip():
                    console.print(f"           [dim][+] OK[/]")
                else:
                    console.print(f"           [dim][*] {preview[:80]}...[/]")
            else:
                console.print(f"           [dim][+] OK[/]")

    # Run the agent
    async def run_agent():
        from ..llm import LLM
        from ..runtime import LocalRuntime
        from ..tools import get_all_tools
        from ..mcp.manager import MCPManager

        # Create LLM and runtime
        llm = LLM(model=model)
        runtime = LocalRuntime()

        # Register general tools
        register_general_tools()

        # Get all available tools
        tools = list(get_all_tools())

        # Connect MCP servers (unless offline mode)
        offline = getattr(args, 'offline', False)
        mcp_manager = None
        if not offline:
            console.print("[dim]Connecting MCP servers...[/]")
            mcp_manager = MCPManager()
            mcp_tools = await mcp_manager.connect_all()
            tools.extend(mcp_tools)
            console.print(f"[dim]MCP tools added: {len(mcp_tools)}[/]")

        console.print(f"[dim]Total tools available: {len(tools)}[/]")
        console.print(f"[dim]LLM-driven agent loop starting...[/]")
        console.print()

        # Create and run agent
        agent = GeneralAgent(
            llm=llm,
            tools=tools,
            runtime=runtime,
            context=context,
            max_iterations=args.max_loops,
        )

        result = await agent.solve(on_message=_solve_on_message)

        # Disconnect MCP cleanly (allow SSL transports to flush)
        if mcp_manager:
            await mcp_manager.disconnect_all()
            await asyncio.sleep(0.2)

        return result

    try:
        result = _quiet_asyncio_run(run_agent())
    except KeyboardInterrupt:
        console.print("\n[yellow]Interrupted by user[/]")
        return
    except Exception as e:
        console.print(f"[red]Error: {e}[/]")
        import traceback
        traceback.print_exc()
        return

    # Display results
    console.print()
    console.print("=" * 60)
    if result["success"]:
        console.print("[bold green]SUCCESS - FLAG FOUND[/]")
    else:
        console.print("[bold yellow]NO FLAG FOUND[/]")
    console.print("=" * 60)

    console.print(f"Run ID:       {result['run_id']}")
    console.print(f"Elapsed:      {result['elapsed_seconds']:.2f}s")
    console.print(f"Ghidra:       {'Available' if result.get('ghidra_available') else 'Not available'}")
    console.print(f"Fallback:     {'Yes' if result.get('used_fallback_plan') else 'No'}")

    if result.get("flags"):
        console.print(f"\n[bold green]Flags ({len(result['flags'])}):[/]")
        for flag in result["flags"]:
            console.print(f"  [bold green]{flag}[/]")

    if result.get("findings"):
        console.print(f"\n[bold]Findings ({len(result['findings'])}):[/]")
        for f in result["findings"][:5]:
            console.print(f"  - {f}")

    if not result.get("flags") and not result.get("findings"):
        console.print("\n[yellow]No flags or findings.[/]")

    console.print(f"\n[dim]Artifacts: {result['out_dir']}/[/]")
    console.print(f"  plan.json      - Solving strategy")
    console.print(f"  tool_log.json  - Tool call history")
    console.print(f"  evidence.json  - Evidence chain")
    console.print(f"  outcome.json   - Found flags")
    console.print(f"  transcript.txt - Full execution log")
    console.print("=" * 60)


def main():
    """Main entry point."""
    from .cli import run_cli
    from .tui import run_tui

    parser, args = parse_arguments()

    # Handle subcommands
    if args.command == "tools":
        handle_tools_command(args)
        return

    if args.command == "mcp":
        handle_mcp_command(args)
        return

    if args.command == "analyze":
        handle_analyze_command(args)
        return

    if args.command == "solve":
        handle_solve_command(args)
        return

    if args.command == "run":
        # Check model configuration
        if not args.model:
            print("Error: No model configured.")
            print("Set PENTESTAGENT_MODEL in .env file or use --model flag.")
            print(
                "Example: PENTESTAGENT_MODEL=gpt-5 or PENTESTAGENT_MODEL=claude-sonnet-4-20250514"
            )
            return

        if not args.target:
            print("Error: --target is required for run mode")
            return

        # Handle playbook or task
        task_description = ""
        mode = "agent"
        if args.playbook:
            from ..playbooks import get_playbook

            try:
                playbook = get_playbook(args.playbook)
                task_description = playbook.get_task()
                mode = getattr(playbook, "mode", "agent")

                # Use playbook's max_loops if defined
                if hasattr(playbook, "max_loops"):
                    args.max_loops = playbook.max_loops

                print(f"Loaded playbook: {playbook.name}")
                print(f"Description: {playbook.description}")
                print(f"Mode: {mode}")
            except ValueError as e:
                print(f"Error: {e}")
                return
        elif args.task:
            task_description = " ".join(args.task)
        else:
            print("Error: Either task (positional) or --playbook is required")
            return

        try:
            asyncio.run(
                run_cli(
                    target=args.target,
                    model=args.model,
                    task=task_description,
                    report=args.report,
                    max_loops=args.max_loops,
                    use_docker=args.docker,
                    mode=mode,
                )
            )
        except KeyboardInterrupt:
            print("\n[!] Interrupted by user.")
        return

    if args.command == "tui":
        # Check model configuration
        if not args.model:
            print("Error: No model configured.")
            print("Set PENTESTAGENT_MODEL in .env file or use --model flag.")
            print(
                "Example: PENTESTAGENT_MODEL=gpt-5 or PENTESTAGENT_MODEL=claude-sonnet-4-20250514"
            )
            return

        run_tui(target=args.target, model=args.model, use_docker=args.docker)
        return

    # If no command provided, default to TUI
    if args.command is None:
        # Ensure a default model is configured; provide a friendly error if not
        if not DEFAULT_MODEL:
            print("Error: No default model configured (PENTESTAGENT_MODEL).")
            print("Set PENTESTAGENT_MODEL in .env file or pass --model on the command line.")
            print(
                "Example: PENTESTAGENT_MODEL=gpt-5 or PENTESTAGENT_MODEL=claude-sonnet-4-20250514"
            )
            return

        run_tui(target=None, model=DEFAULT_MODEL, use_docker=False)
        return


if __name__ == "__main__":
    main()
