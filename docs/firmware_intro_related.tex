\section{Introduction}
Firmware is a strong target for vulnerability-discovery research because it combines high impact with persistent analysis difficulty. Embedded firmware has repeatedly shown broad and long-lived weaknesses \cite{costin2014large}. In practice, source code is often unavailable, updates are slow, and binaries are heterogeneous across architectures/toolchains. Dynamic analysis is often difficult because it depends on firmware rehosting and emulation, where setup and coverage constraints remain substantial \cite{chen2016firmadyne, angelakopoulos2024pandawan}. This creates a gap between what is theoretically analyzable and what can be audited reliably under realistic time and compute budgets.

We focus on firmware because it is both security-relevant and a realistic stress case for practical analysis workflows. This is also why firmware is a good LLM target: if dynamic analysis is bottlenecked by rehosting/emulation, then a static-first workflow can create strong value as a triage and hypothesis-generation layer; dynamic confirmation can be optional or targeted. The core problem is firmware vulnerability discovery on binary-only inputs with heterogeneous tool outputs and incomplete context.

LLMs add value here when used as control components, not stand-alone detectors. Compared with traditional static pipelines, an LLM-driven framework can provide: (i) \textbf{cross-tool orchestration} (routing between disassembly, decompilation, xref, and string analyses under one plan), (ii) \textbf{cross-function and cross-binary reasoning} (tracking evidence across call chains and binaries instead of isolated function-level checks), (iii) \textbf{semantic hypothesis refinement} from pseudocode/assembly context, and (iv) \textbf{evidence-grounded iterative analysis} that updates priorities as new facts appear. Recent work supports parts of this direction: stripped-binary semantic recovery (SymGen, VulBinLLM) \cite{sun2025symgen, liu2025vulbinllm} and tool-mediated planning loops in security agents \cite{shen2025pentestagent}. The main risk is unreliability (hallucination, over-exploration, unstable decisions), which motivates strict execution constraints and direct comparison with non-LLM baselines.

Classical firmware/binary systems provide core primitives (rehosting, symbolic/concolic exploration, fuzzing), but not planner-level resource allocation for long workflows \cite{chen2016firmadyne, stephens2016driller, yun2018qsym}. Concretely, when analysts face $N$ binaries, $M$ candidate functions, and $K$ analysis tools (decompile/xref/taint/strings), these systems do not directly optimize which action should be taken next, when to stop, or when to pivot under weak evidence. For firmware static analysis specifically, Operation Mango demonstrates strong taint-style vulnerability discovery by scaling static data-flow analysis \cite{gibbs2024operationmango}. Recent LLM-binary papers improve semantic recovery, yet leave open reliability and comparative effectiveness on firmware-scale workflows \cite{tan2024llm4decompile, sun2025symgen, liu2025vulbinllm}. Our work is positioned as a reproducible agent-control design evaluated against both LLM-based and traditional analysis baselines.

\paragraph{Research Questions.}
\textbf{RQ1.}
\begin{quote}
\textit{How can we design and build an LLM-driven framework (BinAgent) that effectively leverages planning, scheduling, and pseudocode-level reasoning to improve firmware binary analysis efficiency and end-to-end vulnerability discovery outcomes?}
\end{quote}
\textbf{RQ2.}
\begin{quote}
\textit{How should we define a benchmark and evaluation protocol that meaningfully characterizes BinAgent's capabilities, limitations, and comparative performance against recent LLM-based systems and traditional static firmware-analysis workflows?}
\end{quote}

Our working hypothesis is that the main bottleneck is not the lack of standalone analysis primitives, but weak orchestration across existing ones. We therefore design \textbf{BinAgent} as an LLM-driven control layer that performs planning, tool routing/scheduling, pseudocode reading, and evidence-grounded synthesis over existing firmware-analysis tools.

These research questions motivate two design principles embedded in the introduction above: first, LLM-orchestrated integration of existing methods (planning, dispatch/scheduling, and pseudocode-guided reasoning) as the mechanism for improvement; second, explicit comparison against traditional workflows to explain where and why LLM-driven orchestration adds measurable value.

\paragraph{Planned contributions.}
\begin{enumerate}
\item \textbf{BinAgent framework design:} a planner-centric LLM control layer that coordinates existing static-analysis and reverse-engineering tools through plan--act--observe loops, tool scheduling, and pseudocode-guided reasoning.
\item \textbf{Integration methodology:} a concrete recipe for combining heterogeneous analysis outputs (disassembly, decompilation, xrefs, strings, and notes) into evidence-linked vulnerability hypotheses and decisions.
\item \textbf{Benchmark specification for BinAgent:} a capability-oriented benchmark and protocol covering single-binary and multi-binary firmware tasks, with clear metrics, artifact requirements, and baseline definitions \cite{yang2025pentesteval}.
\end{enumerate}
